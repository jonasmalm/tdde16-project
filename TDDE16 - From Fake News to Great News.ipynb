{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project TDDE16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and splitting"
      ],
      "metadata": {
        "id": "zFlH5skQVK1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Installing packages"
      ],
      "metadata": {
        "id": "EBRjIdyPc3Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "B_FMfaH8rwBF",
        "outputId": "9017f643-cb8d-44ac-a06f-9a65b4150a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.3.0\n",
            "  Downloading pandas-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (10.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.8 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.0) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.0 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g29iW03GsV2S",
        "outputId": "99a44899-3ce1-42dd-c382-97a3db4fbaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.3.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tUEyLvic470",
        "outputId": "de02a2c3-cac6-4553-8c49-0796a8779555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=0db9b3709c91eeb4e94e394af40909f565c057ac28aeb0f296021ef2aeb4f101\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u_iy_pae/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "metadata": {
        "id": "Uow1r3R-d8xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing data\n",
        "https://zenodo.org/record/4561253\n",
        "\n",
        "Label: (1 = Fake, 0 = Real)\n",
        "Switching values for classes to come in same order as previous datasets used\n",
        "Label used post-import: 0 = Fake, 1 = Real\n"
      ],
      "metadata": {
        "id": "Zt4HT82Xw0jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('WELFake_Dataset_cleaned.csv', \n",
        "                   sep = ';', engine = 'python', encoding_errors = 'ignore')\n",
        "data['body'] = data['title'].fillna('') + ' - ' + data['text'].fillna('')\n",
        "data.label = abs(data.label - 1)\n",
        "\n",
        "x = data.body\n",
        "y = data.label\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "y_train = list(y_train)\n",
        "y_test = list(y_test)\n"
      ],
      "metadata": {
        "id": "iiqChiLi76jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Full dataset:')\n",
        "share_fake = (y_train.count(0) + y_test.count(0)) / (len(y_train) + len(y_test))\n",
        "print('{} entries, of which {:.0%} are fake'.format((len(y_train) + len(y_test)), share_fake))\n",
        "\n",
        "print('Train:')\n",
        "share_fake = y_train.count(0) / len(y_train)\n",
        "print('{} entries, of which {:.0%} are fake'.format(len(y_train), share_fake))\n",
        "\n",
        "print('Test:')\n",
        "share_fake = y_test.count(0) / len(y_test)\n",
        "print('{} entries, of which {:.0%} are fake'.format(len(y_test), share_fake))"
      ],
      "metadata": {
        "id": "ulXqmDQH76SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b6ef7d-f369-49b6-e982-abddbcca7541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full dataset:\n",
            "72134 entries, of which 51% are fake\n",
            "Train:\n",
            "50493 entries, of which 51% are fake\n",
            "Test:\n",
            "21641 entries, of which 51% are fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting data"
      ],
      "metadata": {
        "id": "zey-tt9uVJSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data['label'].value_counts().plot(kind = 'bar')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "BsI-vA-uQ-_G",
        "outputId": "7b6965b4-b083-4fb3-f587-bc605c7624b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f18dccb3210>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "1J8RmbEQXzCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing model"
      ],
      "metadata": {
        "id": "rKLmZkVZZaNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words = 'english')\n",
        "nb = MultinomialNB()\n",
        "\n",
        "pipe_naive = Pipeline(steps = [\n",
        "                               ('vec', tfidf),\n",
        "                               ('nb', nb)\n",
        "                               ])\n",
        "\n",
        "model_naive = pipe_naive.fit(X = x_train, y = y_train)"
      ],
      "metadata": {
        "id": "8B9LQvsMRQYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_nb = model_naive.predict(x_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_nb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXbH1iKQYFpC",
        "outputId": "35a86072-cb35-462d-cdd8-d064db12cfe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88     11131\n",
            "           1       0.87      0.87      0.87     10510\n",
            "\n",
            "    accuracy                           0.87     21641\n",
            "   macro avg       0.87      0.87      0.87     21641\n",
            "weighted avg       0.87      0.87      0.87     21641\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting"
      ],
      "metadata": {
        "id": "8ApiSlv4Zb4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating feature lookup"
      ],
      "metadata": {
        "id": "04HFolKtXNHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Generate total counts in training corpus\n",
        "dt_matrix = tfidf.fit_transform(x_train)\n",
        "tfidf_sum = dt_matrix.sum(axis = 0)\n",
        "tfidf_sum = np.transpose(tfidf_sum).tolist()\n",
        "tfidf_sum = [e[0] for e in tfidf_sum]\n",
        "\n",
        "cvec = CountVectorizer(stop_words = 'english')\n",
        "count_matrix = cvec.fit_transform(x_train)\n",
        "c_sum = count_matrix.sum(axis = 0)\n",
        "c_sum = np.transpose(c_sum).tolist()\n",
        "c_sum = [e[0] for e in c_sum]\n",
        "\n",
        "\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "\n",
        "real_log_prob = nb.feature_log_prob_[1]\n",
        "fake_log_prob = nb.feature_log_prob_[0]\n",
        "\n",
        "# real - fake\n",
        "# > 0 --> Important for real (Positive contribution to predicting OR)\n",
        "# < 0 --> Important for fake\n",
        "delta_log_prob = real_log_prob - fake_log_prob\n",
        "delta_adjusted_tfidf = [d * t for (d, t) in zip(delta_log_prob, tfidf_sum)]\n",
        "delta_adjusted_count = [d * c for (d, c) in zip(delta_log_prob, c_sum)]\n",
        "\n",
        "# Converting log prob to normal prob\n",
        "real_prob = np.exp(real_log_prob)\n",
        "fake_prob = np.exp(fake_log_prob)\n",
        "delta_prob = real_prob - fake_prob\n",
        "\n",
        "# count = delta_adjusted_count,\n",
        "d = dict(\n",
        "    word = feature_names, \n",
        "    real = real_log_prob, \n",
        "    fake = fake_log_prob, \n",
        "    delta = delta_log_prob,\n",
        "    importance_tfidf = delta_adjusted_tfidf,\n",
        "    importance_count = delta_adjusted_count\n",
        "    )\n",
        "\n",
        "nb_lookup = pd.DataFrame(data = d)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lh7v3NZuZdr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_lookup.sort_values(by = 'importance_count', ascending = False).iloc[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yfR0c3Epk6Cp",
        "outputId": "a5ad7cb8-c0c7-42ff-dae2-16d8e0d690f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bac08f37-38e9-4f04-b713-9f518cb11ec8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>real</th>\n",
              "      <th>fake</th>\n",
              "      <th>delta</th>\n",
              "      <th>importance_tfidf</th>\n",
              "      <th>importance_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>155877</th>\n",
              "      <td>said</td>\n",
              "      <td>-5.805900</td>\n",
              "      <td>-7.020278</td>\n",
              "      <td>1.214378</td>\n",
              "      <td>2113.249006</td>\n",
              "      <td>199488.303753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120014</th>\n",
              "      <td>mr</td>\n",
              "      <td>-6.501381</td>\n",
              "      <td>-8.344160</td>\n",
              "      <td>1.842779</td>\n",
              "      <td>1427.524332</td>\n",
              "      <td>93174.584919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151427</th>\n",
              "      <td>reuters</td>\n",
              "      <td>-6.752596</td>\n",
              "      <td>-10.404903</td>\n",
              "      <td>3.652307</td>\n",
              "      <td>1948.142552</td>\n",
              "      <td>78082.669572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182462</th>\n",
              "      <td>trumps</td>\n",
              "      <td>-7.160686</td>\n",
              "      <td>-8.424605</td>\n",
              "      <td>1.263919</td>\n",
              "      <td>559.332607</td>\n",
              "      <td>24114.315874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123983</th>\n",
              "      <td>new</td>\n",
              "      <td>-6.928168</td>\n",
              "      <td>-7.334097</td>\n",
              "      <td>0.405929</td>\n",
              "      <td>294.704906</td>\n",
              "      <td>22738.527591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85256</th>\n",
              "      <td>house</td>\n",
              "      <td>-6.901058</td>\n",
              "      <td>-7.590313</td>\n",
              "      <td>0.689255</td>\n",
              "      <td>463.499749</td>\n",
              "      <td>22392.522369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169146</th>\n",
              "      <td>states</td>\n",
              "      <td>-7.070092</td>\n",
              "      <td>-7.647862</td>\n",
              "      <td>0.577770</td>\n",
              "      <td>340.846475</td>\n",
              "      <td>21459.534233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169076</th>\n",
              "      <td>state</td>\n",
              "      <td>-6.941765</td>\n",
              "      <td>-7.419737</td>\n",
              "      <td>0.477972</td>\n",
              "      <td>332.800806</td>\n",
              "      <td>20847.701353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185899</th>\n",
              "      <td>united</td>\n",
              "      <td>-7.149787</td>\n",
              "      <td>-7.852217</td>\n",
              "      <td>0.702430</td>\n",
              "      <td>366.422655</td>\n",
              "      <td>20828.453177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76645</th>\n",
              "      <td>government</td>\n",
              "      <td>-7.095374</td>\n",
              "      <td>-7.683032</td>\n",
              "      <td>0.587658</td>\n",
              "      <td>336.800247</td>\n",
              "      <td>20181.351620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192319</th>\n",
              "      <td>washington</td>\n",
              "      <td>-7.239537</td>\n",
              "      <td>-8.212974</td>\n",
              "      <td>0.973438</td>\n",
              "      <td>427.534923</td>\n",
              "      <td>19530.080840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116800</th>\n",
              "      <td>minister</td>\n",
              "      <td>-7.457444</td>\n",
              "      <td>-9.272784</td>\n",
              "      <td>1.815340</td>\n",
              "      <td>540.390593</td>\n",
              "      <td>18269.579309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150475</th>\n",
              "      <td>republican</td>\n",
              "      <td>-7.036676</td>\n",
              "      <td>-7.645941</td>\n",
              "      <td>0.609265</td>\n",
              "      <td>367.540959</td>\n",
              "      <td>17266.570590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120115</th>\n",
              "      <td>ms</td>\n",
              "      <td>-7.941925</td>\n",
              "      <td>-9.745777</td>\n",
              "      <td>1.803852</td>\n",
              "      <td>329.933672</td>\n",
              "      <td>17261.057349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135535</th>\n",
              "      <td>percent</td>\n",
              "      <td>-7.318981</td>\n",
              "      <td>-8.129947</td>\n",
              "      <td>0.810967</td>\n",
              "      <td>344.770395</td>\n",
              "      <td>16161.756188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38126</th>\n",
              "      <td>china</td>\n",
              "      <td>-7.435756</td>\n",
              "      <td>-8.772989</td>\n",
              "      <td>1.337232</td>\n",
              "      <td>441.823007</td>\n",
              "      <td>15882.309860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126300</th>\n",
              "      <td>north</td>\n",
              "      <td>-7.340915</td>\n",
              "      <td>-8.439673</td>\n",
              "      <td>1.098758</td>\n",
              "      <td>421.751839</td>\n",
              "      <td>15683.666677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160140</th>\n",
              "      <td>senate</td>\n",
              "      <td>-7.325469</td>\n",
              "      <td>-8.462262</td>\n",
              "      <td>1.136793</td>\n",
              "      <td>439.041720</td>\n",
              "      <td>15193.237017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182934</th>\n",
              "      <td>tuesday</td>\n",
              "      <td>-7.537476</td>\n",
              "      <td>-8.822090</td>\n",
              "      <td>1.284614</td>\n",
              "      <td>387.452620</td>\n",
              "      <td>14480.171915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198401</th>\n",
              "      <td>york</td>\n",
              "      <td>-7.593195</td>\n",
              "      <td>-8.373812</td>\n",
              "      <td>0.780617</td>\n",
              "      <td>254.294681</td>\n",
              "      <td>14445.324934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179627</th>\n",
              "      <td>told</td>\n",
              "      <td>-7.337762</td>\n",
              "      <td>-7.881266</td>\n",
              "      <td>0.543505</td>\n",
              "      <td>248.152693</td>\n",
              "      <td>14260.477397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128650</th>\n",
              "      <td>officials</td>\n",
              "      <td>-7.558363</td>\n",
              "      <td>-8.423673</td>\n",
              "      <td>0.865309</td>\n",
              "      <td>284.474236</td>\n",
              "      <td>14231.740542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178386</th>\n",
              "      <td>thursday</td>\n",
              "      <td>-7.543907</td>\n",
              "      <td>-8.897984</td>\n",
              "      <td>1.354077</td>\n",
              "      <td>399.853558</td>\n",
              "      <td>13926.686681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141242</th>\n",
              "      <td>president</td>\n",
              "      <td>-6.733552</td>\n",
              "      <td>-6.951393</td>\n",
              "      <td>0.217840</td>\n",
              "      <td>208.111622</td>\n",
              "      <td>13667.747668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45170</th>\n",
              "      <td>court</td>\n",
              "      <td>-7.352300</td>\n",
              "      <td>-8.137441</td>\n",
              "      <td>0.785140</td>\n",
              "      <td>325.401306</td>\n",
              "      <td>13524.827929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100799</th>\n",
              "      <td>korea</td>\n",
              "      <td>-7.447859</td>\n",
              "      <td>-9.184819</td>\n",
              "      <td>1.736960</td>\n",
              "      <td>528.040408</td>\n",
              "      <td>13487.494824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192984</th>\n",
              "      <td>wednesday</td>\n",
              "      <td>-7.555366</td>\n",
              "      <td>-8.838510</td>\n",
              "      <td>1.283144</td>\n",
              "      <td>380.223343</td>\n",
              "      <td>13465.314242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159551</th>\n",
              "      <td>security</td>\n",
              "      <td>-7.411671</td>\n",
              "      <td>-8.116613</td>\n",
              "      <td>0.704942</td>\n",
              "      <td>282.447261</td>\n",
              "      <td>13122.492198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175171</th>\n",
              "      <td>tax</td>\n",
              "      <td>-7.405286</td>\n",
              "      <td>-8.531651</td>\n",
              "      <td>1.126365</td>\n",
              "      <td>402.492993</td>\n",
              "      <td>13115.394437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70923</th>\n",
              "      <td>friday</td>\n",
              "      <td>-7.569813</td>\n",
              "      <td>-8.784565</td>\n",
              "      <td>1.214753</td>\n",
              "      <td>360.221573</td>\n",
              "      <td>13037.939748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118542</th>\n",
              "      <td>monday</td>\n",
              "      <td>-7.646918</td>\n",
              "      <td>-8.888627</td>\n",
              "      <td>1.241709</td>\n",
              "      <td>338.626796</td>\n",
              "      <td>12224.620538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180669</th>\n",
              "      <td>trade</td>\n",
              "      <td>-7.756785</td>\n",
              "      <td>-9.112668</td>\n",
              "      <td>1.355882</td>\n",
              "      <td>322.974098</td>\n",
              "      <td>11750.075477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88714</th>\n",
              "      <td>including</td>\n",
              "      <td>-7.742321</td>\n",
              "      <td>-8.425004</td>\n",
              "      <td>0.682682</td>\n",
              "      <td>197.598505</td>\n",
              "      <td>11522.312745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197796</th>\n",
              "      <td>year</td>\n",
              "      <td>-7.358165</td>\n",
              "      <td>-7.727916</td>\n",
              "      <td>0.369751</td>\n",
              "      <td>176.934986</td>\n",
              "      <td>11448.226316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169119</th>\n",
              "      <td>statement</td>\n",
              "      <td>-7.599916</td>\n",
              "      <td>-8.461944</td>\n",
              "      <td>0.862028</td>\n",
              "      <td>272.056160</td>\n",
              "      <td>11039.994586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193010</th>\n",
              "      <td>week</td>\n",
              "      <td>-7.616217</td>\n",
              "      <td>-8.278100</td>\n",
              "      <td>0.661883</td>\n",
              "      <td>219.042336</td>\n",
              "      <td>10835.691626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167930</th>\n",
              "      <td>spokesman</td>\n",
              "      <td>-7.948490</td>\n",
              "      <td>-9.750158</td>\n",
              "      <td>1.801668</td>\n",
              "      <td>327.456095</td>\n",
              "      <td>10175.818920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69550</th>\n",
              "      <td>foreign</td>\n",
              "      <td>-7.581286</td>\n",
              "      <td>-8.295382</td>\n",
              "      <td>0.714096</td>\n",
              "      <td>240.526913</td>\n",
              "      <td>10173.010933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121423</th>\n",
              "      <td>myanmar</td>\n",
              "      <td>-8.387022</td>\n",
              "      <td>-12.701048</td>\n",
              "      <td>4.314026</td>\n",
              "      <td>436.437542</td>\n",
              "      <td>9922.260523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49189</th>\n",
              "      <td>deal</td>\n",
              "      <td>-7.670952</td>\n",
              "      <td>-8.506463</td>\n",
              "      <td>0.835510</td>\n",
              "      <td>247.454947</td>\n",
              "      <td>9544.869709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63322</th>\n",
              "      <td>european</td>\n",
              "      <td>-7.941377</td>\n",
              "      <td>-9.336015</td>\n",
              "      <td>1.394638</td>\n",
              "      <td>273.580715</td>\n",
              "      <td>9359.414440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167029</th>\n",
              "      <td>south</td>\n",
              "      <td>-7.885429</td>\n",
              "      <td>-8.917726</td>\n",
              "      <td>1.032296</td>\n",
              "      <td>232.953829</td>\n",
              "      <td>9281.375774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133925</th>\n",
              "      <td>party</td>\n",
              "      <td>-7.308720</td>\n",
              "      <td>-7.684401</td>\n",
              "      <td>0.375682</td>\n",
              "      <td>188.466826</td>\n",
              "      <td>9167.009072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174376</th>\n",
              "      <td>talks</td>\n",
              "      <td>-7.981968</td>\n",
              "      <td>-9.844971</td>\n",
              "      <td>1.863003</td>\n",
              "      <td>324.557195</td>\n",
              "      <td>8914.469658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104040</th>\n",
              "      <td>leaders</td>\n",
              "      <td>-7.899927</td>\n",
              "      <td>-8.889643</td>\n",
              "      <td>0.989715</td>\n",
              "      <td>222.628005</td>\n",
              "      <td>8772.836339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63162</th>\n",
              "      <td>eu</td>\n",
              "      <td>-7.817668</td>\n",
              "      <td>-9.481335</td>\n",
              "      <td>1.663667</td>\n",
              "      <td>352.367265</td>\n",
              "      <td>8759.204283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41734</th>\n",
              "      <td>committee</td>\n",
              "      <td>-7.777874</td>\n",
              "      <td>-8.670569</td>\n",
              "      <td>0.892695</td>\n",
              "      <td>233.395163</td>\n",
              "      <td>8674.313641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141698</th>\n",
              "      <td>prime</td>\n",
              "      <td>-8.007412</td>\n",
              "      <td>-9.573614</td>\n",
              "      <td>1.566202</td>\n",
              "      <td>278.345159</td>\n",
              "      <td>8659.528536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91733</th>\n",
              "      <td>iran</td>\n",
              "      <td>-7.650435</td>\n",
              "      <td>-8.546266</td>\n",
              "      <td>0.895831</td>\n",
              "      <td>266.049268</td>\n",
              "      <td>8358.101424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118924</th>\n",
              "      <td>month</td>\n",
              "      <td>-7.986842</td>\n",
              "      <td>-8.847940</td>\n",
              "      <td>0.861098</td>\n",
              "      <td>184.063842</td>\n",
              "      <td>8241.572915</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bac08f37-38e9-4f04-b713-9f518cb11ec8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bac08f37-38e9-4f04-b713-9f518cb11ec8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bac08f37-38e9-4f04-b713-9f518cb11ec8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              word      real  ...  importance_tfidf  importance_count\n",
              "155877        said -5.805900  ...       2113.249006     199488.303753\n",
              "120014          mr -6.501381  ...       1427.524332      93174.584919\n",
              "151427     reuters -6.752596  ...       1948.142552      78082.669572\n",
              "182462      trumps -7.160686  ...        559.332607      24114.315874\n",
              "123983         new -6.928168  ...        294.704906      22738.527591\n",
              "85256        house -6.901058  ...        463.499749      22392.522369\n",
              "169146      states -7.070092  ...        340.846475      21459.534233\n",
              "169076       state -6.941765  ...        332.800806      20847.701353\n",
              "185899      united -7.149787  ...        366.422655      20828.453177\n",
              "76645   government -7.095374  ...        336.800247      20181.351620\n",
              "192319  washington -7.239537  ...        427.534923      19530.080840\n",
              "116800    minister -7.457444  ...        540.390593      18269.579309\n",
              "150475  republican -7.036676  ...        367.540959      17266.570590\n",
              "120115          ms -7.941925  ...        329.933672      17261.057349\n",
              "135535     percent -7.318981  ...        344.770395      16161.756188\n",
              "38126        china -7.435756  ...        441.823007      15882.309860\n",
              "126300       north -7.340915  ...        421.751839      15683.666677\n",
              "160140      senate -7.325469  ...        439.041720      15193.237017\n",
              "182934     tuesday -7.537476  ...        387.452620      14480.171915\n",
              "198401        york -7.593195  ...        254.294681      14445.324934\n",
              "179627        told -7.337762  ...        248.152693      14260.477397\n",
              "128650   officials -7.558363  ...        284.474236      14231.740542\n",
              "178386    thursday -7.543907  ...        399.853558      13926.686681\n",
              "141242   president -6.733552  ...        208.111622      13667.747668\n",
              "45170        court -7.352300  ...        325.401306      13524.827929\n",
              "100799       korea -7.447859  ...        528.040408      13487.494824\n",
              "192984   wednesday -7.555366  ...        380.223343      13465.314242\n",
              "159551    security -7.411671  ...        282.447261      13122.492198\n",
              "175171         tax -7.405286  ...        402.492993      13115.394437\n",
              "70923       friday -7.569813  ...        360.221573      13037.939748\n",
              "118542      monday -7.646918  ...        338.626796      12224.620538\n",
              "180669       trade -7.756785  ...        322.974098      11750.075477\n",
              "88714    including -7.742321  ...        197.598505      11522.312745\n",
              "197796        year -7.358165  ...        176.934986      11448.226316\n",
              "169119   statement -7.599916  ...        272.056160      11039.994586\n",
              "193010        week -7.616217  ...        219.042336      10835.691626\n",
              "167930   spokesman -7.948490  ...        327.456095      10175.818920\n",
              "69550      foreign -7.581286  ...        240.526913      10173.010933\n",
              "121423     myanmar -8.387022  ...        436.437542       9922.260523\n",
              "49189         deal -7.670952  ...        247.454947       9544.869709\n",
              "63322     european -7.941377  ...        273.580715       9359.414440\n",
              "167029       south -7.885429  ...        232.953829       9281.375774\n",
              "133925       party -7.308720  ...        188.466826       9167.009072\n",
              "174376       talks -7.981968  ...        324.557195       8914.469658\n",
              "104040     leaders -7.899927  ...        222.628005       8772.836339\n",
              "63162           eu -7.817668  ...        352.367265       8759.204283\n",
              "41734    committee -7.777874  ...        233.395163       8674.313641\n",
              "141698       prime -8.007412  ...        278.345159       8659.528536\n",
              "91733         iran -7.650435  ...        266.049268       8358.101424\n",
              "118924       month -7.986842  ...        184.063842       8241.572915\n",
              "\n",
              "[50 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing individual articles (Interesting, not part of analysis)"
      ],
      "metadata": {
        "id": "vPjVAqmVW-EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_transformed = tfidf.fit_transform(x_train)\n",
        "\n",
        "probabilities = nb.predict_proba(x_train_transformed)\n",
        "probs = [\n",
        "         [p[0] for p in probabilities],\n",
        "         [p[1] for p in probabilities]\n",
        "]\n",
        "\n",
        "delta_prob = [p1 - p0 for (p0, p1) in zip(probs[0], probs[1])]\n",
        "\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "d = dict(\n",
        "    text = x_train,\n",
        "    gold_label = y_train,\n",
        "    prob_real = probs[1],\n",
        "    prob_fake = probs[0],\n",
        "    prob_delta = delta_prob\n",
        ")\n",
        "\n",
        "nb_prob_lookup = pd.DataFrame(data = d)\n",
        "\n"
      ],
      "metadata": {
        "id": "0wZ17l8LoEEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_new_lines(s, width = 75):\n",
        "  tmp = list(s)\n",
        "  index = 0\n",
        "  counter = 0\n",
        "  for c in s:\n",
        "    if counter > width and c == ' ':\n",
        "      tmp[index] = '\\n'\n",
        "      counter = 0\n",
        "    else:\n",
        "      counter = counter + 1\n",
        "    index = index + 1\n",
        "  return ''.join(tmp)\n",
        "\n",
        "test = add_new_lines('This would select the first two rows of the data frame, then return the rows out of the first two rows that have a value for the col3 equal to 7. Point being you want to use iterrows only in very very specific situations. Otherwise, the solution can be vectorized.')\n",
        "\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iid_bJl2xZYI",
        "outputId": "78ab8e58-78fb-4189-fabf-e326ab96d848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This would select the first two rows of the data frame, then return the rows\n",
            "out of the first two rows that have a value for the col3 equal to 7. Point being\n",
            "you want to use iterrows only in very very specific situations. Otherwise, the\n",
            "solution can be vectorized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def important_words(text, real = True, n = 10, use_ngrams = False):\n",
        "  words = text.split(' ')\n",
        "\n",
        "  if use_ngrams:\n",
        "    tmpvec = CountVectorizer(stop_words = 'english', ngram_range = (2,3))\n",
        "    tmpvec.fit_transform([text])\n",
        "    words = list(tmpvec.get_feature_names_out())\n",
        "\n",
        "  filtered_df = nb_lookup[nb_lookup['word'].isin(words)]\n",
        "  if real:\n",
        "    top_n = filtered_df.nlargest(n, 'delta')\n",
        "  else:\n",
        "    top_n = filtered_df.nsmallest(n, 'delta')\n",
        "\n",
        "  features = list(top_n['word'])\n",
        "  deltas = list(top_n['delta'])\n",
        "  p_real = list(top_n['real'])\n",
        "  p_fake = list(top_n['fake'])\n",
        "\n",
        "  counts = []\n",
        "  for word in features:\n",
        "    counts.append(words.count(word))\n",
        "\n",
        "  retval = list(zip(features, deltas, p_real, p_fake, counts))\n",
        "\n",
        "  return sorted(retval, key = lambda e : e[1], reverse = real)\n"
      ],
      "metadata": {
        "id": "ehp11b2zydMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Prints nicely, used in explain_predictions\n",
        "def get_print_words(zipped_list):\n",
        "  retval = 'Word' + ' '*17 + 'Count    P(Real)  P(Fake)  Delta'\n",
        "  for word, delta, p_real, p_fake, count in zipped_list:\n",
        "    retval = retval + '\\n{:20} {:<2}      {: .2%}   {: .2%}   {: .2%}'.format(word, count, p_real, p_fake, delta)\n",
        "  return retval\n",
        "\n",
        "# Prints predictions showing which words are most important\n",
        "def explain_predictions(percentile, credible = True, int_labels = False, n = 10):\n",
        "  start_pos = math.ceil(percentile * len(nb_prob_lookup))\n",
        "  end_pos = start_pos + n\n",
        "\n",
        "\n",
        "  for i, row in nb_prob_lookup.sort_values(by = ['prob_delta'], ascending = not credible).iloc[start_pos:end_pos].iterrows():\n",
        "    \n",
        "    gold_label = row['gold_label']\n",
        "    if int_labels:\n",
        "      gold_label = 'Real' if gold_label == 1 else 'Fake'\n",
        "\n",
        "    print('\\n\\n==============================================  Row {}  =============================================='.format(i))\n",
        "    print('Gold label: ' + gold_label)\n",
        "    print('P(Real) = {:.0%}, P(Fake) = {:.0%}'.format(row['prob_real'], row['prob_fake']))\n",
        "    print('===   Text    === \\n' + add_new_lines(row['text'], width = 150))\n",
        "    print('\\n=== Top words contributing to P(Real)  ===')\n",
        "    print(get_print_words(important_words(row['text'], real = True)))\n",
        "    print('\\n=== Top words contributing to P(False) ===')\n",
        "    print(get_print_words(important_words(row['text'], real = False)))\n",
        "  "
      ],
      "metadata": {
        "id": "hJNm7BB1q7og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain_predictions(0.4, int_labels = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWUMnRfT2PlH",
        "outputId": "f02f16bd-c660-4c3e-efd0-948c7c1d5089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==============================================  Row 442  ==============================================\n",
            "Gold label: Real\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "Man Sentenced to 12 Years in Beating Death of Transgender Woman - The New York Times - A Brooklyn man who beat a transgender woman to death after he had\n",
            "started flirting with her was sentenced to 12 years in prison on Tuesday, a penalty the victims family said was too light. The man, James Dixon, 25, pleaded\n",
            "guilty to manslaughter this month in State Supreme Court in Manhattan, admitting he attacked the woman, Islan Nettles, on a street in Harlem just after\n",
            "midnight on Aug. 17, 2013, knocking her to the pavement with a punch, then hitting her again as she lay on the sidewalk. Ms. Nettles, 21, an assistant at\n",
            "a fashion company, died five days later of head injuries, prompting vigils and protests by transgender people who said her death was emblematic of the violence\n",
            "they often face because of their sexual identity. While Ms. Nettles was still in a coma, Mr. Dixon turned himself in to the police. In written and videotaped\n",
            "statements, he told detectives that he had started flirting with Ms. Nettles, unaware she was transgender, after meeting her and her two friends on the\n",
            "street. He said he became enraged and attacked her when his friends began mocking him for trying to pick up a transgender woman. I just didnt want to be\n",
            "fooled, he said in a videotaped statement. The victims mother, Delores Nettles, told the court she believed 12 years was too light a sentence, given the\n",
            "brutal beating her daughter took. He can go home after those 12 years and see his family, she said in tears. Its not fair.  Turning to Mr. Dixon, she said:\n",
            "How can you sleep at night? How can you rest? I cant rest.  Mr. Dixon showed no emotion. He declined to make a statement before Justice Daniel P. Conviser\n",
            "pronounced the sentence, which had been negotiated with the judge on the eve of the trial in return for a guilty plea. Prosecutors objected to the deal\n",
            "they had sought a sentence of at least 17 years.\n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 5        0.30%    0.09%    0.21%\n",
            "court                1        0.06%    0.03%    0.03%\n",
            "statement            1        0.05%    0.02%    0.03%\n",
            "told                 2        0.07%    0.04%    0.03%\n",
            "deal                 1        0.05%    0.02%    0.03%\n",
            "month                1        0.03%    0.01%    0.02%\n",
            "meeting              1        0.04%    0.02%    0.02%\n",
            "declined             1        0.02%    0.00%    0.01%\n",
            "later                1        0.03%    0.02%    0.01%\n",
            "sought               1        0.01%    0.00%    0.01%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "just                 2        0.04%    0.09%   -0.05%\n",
            "people               1        0.08%    0.10%   -0.03%\n",
            "man                  1        0.02%    0.04%   -0.02%\n",
            "woman                1        0.01%    0.03%   -0.02%\n",
            "make                 1        0.04%    0.05%   -0.01%\n",
            "want                 1        0.03%    0.04%   -0.01%\n",
            "family               1        0.02%    0.03%   -0.01%\n",
            "sexual               1        0.01%    0.02%   -0.01%\n",
            "friends              2        0.01%    0.02%   -0.01%\n",
            "trying               1        0.02%    0.02%   -0.01%\n",
            "\n",
            "\n",
            "==============================================  Row 2357  ==============================================\n",
            "Gold label: Fake\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "Planned Parenthood: Abortion pill usage now rivals surgery - Planned Parenthood: Abortion pill usage now rivals surgery October 31, 2016 The Planned Parenthood\n",
            "logo is pictured outside a clinic in Boston, Massachusetts, June 27, 2014. REUTERS/Dominick Reuter \n",
            "Abortion pill usage has almost overtaken surgical alternatives\n",
            "as legalization of infanticide faces a major chance of repeal. in election 2016. Two medications used to induce abortion won US approval in 2000, but Pro-Life\n",
            "activists successfully implemented legislative restrictions. In 2014: Abortion medication was used in 43 percent of pregnancy terminations at Planned Parenthood\n",
            "clinics. in 2010: Up from 35 percent according to previously unreported figures by Planned Parenthood. In Ohio, Texas and North Dakota: Demand for medication\n",
            "abortions tripled in the last several months to as much as 30 percent of all procedures in some clinics. States with no restrictions: Up to 55 percent in\n",
            "Michigan, 64 percent in Iowa. Studies: Drug induced abortions kill the child up to 95 percent of the time. The abortion pill was approved in France in 1988.\n",
            "Guttmacher Institute: Pill used in 91 percent of abortions in Finland, 80 percent in Scotland. 1 million of the more than 2.75 million U.S. women who have\n",
            "used the abortion pill received it from Planned Parenthood. Federal data: Overall U.S. abortion rates have dropped to a low of 16.9 terminations per 1,000\n",
            "women aged15-44 in 2011, down from 19.4 per 1,000 in 2008. U.S. Food and Drug Administration allows abortion pills to be used as far as 10 weeks into pregnancies.\n",
            "Planned Parenthood said both types of abortion typically cost from $300 to $1,000. \n",
            "(NEW YORK CITY) American women are ending pregnancies with medication\n",
            "almost as often as with surgery, marking a turning point for abortion in the United States, data reviewed by Reuters shows. \n",
            "The watershed comes amid an\n",
            "overall decline in abortion, a choice that remains politically charged in the United States, sparking a fiery exchange in the final debate between presidential\n",
            "nominees Hillary Clinton and Donald Trump. \n",
            "When the two medications used to induce abortion won U.S. approval 16 years ago, the method was expected to\n",
            "quickly overtake the surgical option, as it has in much of Europe. But U.S. abortion opponents persuaded lawmakers in many states to put restrictions on\n",
            "their use. \n",
            "Although many limitations remain, innovative dispensing efforts in some states, restricted access to surgical abortions in others and greater\n",
            "awareness boosted medication abortions to 43 percent of pregnancy terminations at Planned Parenthood clinics, the nation's single largest provider, in 2014,\n",
            "up from 35 percent in 2010, according to previously unreported figures from the nonprofit. \n",
            "The national rate is likely even higher now because of new federal\n",
            "prescribing guidelines that took effect in March. In three states most impacted by that change - Ohio, Texas and North Dakota - demand for medication abortions\n",
            "tripled in the last several months to as much as 30 percent of all procedures in some clinics, according to data gathered by Reuters from clinics, state\n",
            "health departments and Planned Parenthood affiliates.\n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 1        0.30%    0.09%    0.21%\n",
            "states               2        0.09%    0.05%    0.04%\n",
            "percent              11       0.07%    0.03%    0.04%\n",
            "state                1        0.10%    0.06%    0.04%\n",
            "new                  1        0.10%    0.07%    0.03%\n",
            "lawmakers            1        0.03%    0.01%    0.02%\n",
            "expected             1        0.03%    0.01%    0.02%\n",
            "presidential         1        0.05%    0.04%    0.01%\n",
            "national             1        0.05%    0.04%    0.01%\n",
            "federal              1        0.04%    0.03%    0.01%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "2016                 1        0.03%    0.06%   -0.04%\n",
            "women                3        0.03%    0.05%   -0.02%\n",
            "point                1        0.02%    0.03%   -0.01%\n",
            "child                1        0.01%    0.02%   -0.01%\n",
            "election             1        0.06%    0.07%   -0.01%\n",
            "comes                1        0.01%    0.02%   -0.01%\n",
            "kill                 1        0.01%    0.01%   -0.01%\n",
            "single               1        0.01%    0.01%   -0.00%\n",
            "used                 6        0.02%    0.03%   -0.00%\n",
            "16                   1        0.01%    0.01%   -0.00%\n",
            "\n",
            "\n",
            "==============================================  Row 905  ==============================================\n",
            "Gold label: Real\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "Violence prompts U.S. Congress to discuss militant threats - WASHINGTON (Reuters) - A U.S. congressional panel next month will hold a hearing on violent\n",
            "extremism, including threats from domestic militants, following a white supremacist rally in Charlottesville, Virginia, that turned deadly. The chairman\n",
            "of the U.S. House of Representatives Homeland Security Committee, Republican Michael McCaul, announced the Sept. 12 hearing in a letter to the panels top\n",
            "Democrat, Bennie Thompson. The committee holds a hearing once a year, around the anniversary of the Sept. 11, 2001, attacks, to discuss worldwide threats.\n",
            "A committee aide said the Charlottesville protests had  prompted the decision to broaden the hearing to include threats from domestic militants. But Thompson\n",
            "said the move was not adequate or appropriate to address his request for a hearing on threats from white supremacists and neo-Nazi groups. The September\n",
            "12 hearing to cover worldwide threats is an annual hearing that was already scheduled prior to the domestic attacks this weekend, Thompson said. It will\n",
            "not allow us to go into the depth necessary to address the far-ranging and multifaceted aspects of the threat posed by domestic terrorist threats from white\n",
            "supremacist and neo-Nazi groups. The Homeland Security Committee will invite leaders of the Homeland Security Department, the Federal Bureau of Investigation\n",
            "and the National Counterterrorism Center, McCaul said. We must stand together and reject racism, bigotry, and prejudice, including the hateful ideologies\n",
            "promoted by neo-Nazis, the KKK, and all other white supremacy groups, McCaul wrote in his response to Democrats request for a hearing. A 32-year-old woman\n",
            "was killed on Saturday in Charlottesville when a car plowed into a rival protest to white supremacist demonstrators. A 20-year-old Ohio man said to have\n",
            "harbored Nazi sympathies has been charged with murder.  President Donald Trump on Tuesday said both sides were to blame for the violence, drawing condemnation\n",
            "from both fellow Republicans and Democrats for failing to single out the white nationalists. \n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 4        0.30%    0.09%    0.21%\n",
            "committee            2        0.04%    0.02%    0.02%\n",
            "leaders              1        0.04%    0.01%    0.02%\n",
            "including            2        0.04%    0.02%    0.02%\n",
            "month                1        0.03%    0.01%    0.02%\n",
            "decision             1        0.03%    0.02%    0.02%\n",
            "killed               1        0.03%    0.02%    0.01%\n",
            "chairman             1        0.02%    0.01%    0.01%\n",
            "attacks              1        0.03%    0.02%    0.01%\n",
            "congressional        1        0.02%    0.01%    0.01%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "man                  1        0.02%    0.04%   -0.02%\n",
            "woman                1        0.01%    0.03%   -0.02%\n",
            "rally                1        0.01%    0.02%   -0.01%\n",
            "stand                1        0.01%    0.02%   -0.01%\n",
            "blame                1        0.01%    0.01%   -0.01%\n",
            "hateful              1        0.00%    0.01%   -0.01%\n",
            "violent              1        0.01%    0.01%   -0.01%\n",
            "cover                1        0.01%    0.01%   -0.01%\n",
            "protest              1        0.01%    0.02%   -0.01%\n",
            "single               1        0.01%    0.01%   -0.00%\n",
            "\n",
            "\n",
            "==============================================  Row 6983  ==============================================\n",
            "Gold label: Real\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "Donald Trump: Obamacare One of the Biggest Broken Promises in History of Politics - Breitbart - President Donald Trump thundered against Obamacare in Ohio\n",
            "on Wednesday, calling it one of the biggest broken promises in the history of politics. [The president met with families who had their lives upended by\n",
            "Obamacare, addressing reporters at the Ohio airport before his infrastructure speech.  Obamacare is dead, Ive been saying it for a long time, everybody\n",
            "knows it, everybody that wants to report fairly about it does, Trump said. He delivered a speech after Anthem announced that they were pulling out of the\n",
            "health care exchanges in Ohio, due to prohibitive costs to the company. Thats it,  . Wave  Trump said. What a mess.  Trump commented during his speech that\n",
            "Democrats suffered immensely politically for their actions, as they were voted out of office. Thats why they lost the House. They lost the Senate. They\n",
            "lost the White House, he said. Trump warned both parties in Congress to pass an Obamacare replacement quickly, and commented that Senate Majority Mitch\n",
            "McConnell was working to get a proposal finished as soon as possible. He specifically called out Democrats for trying to stop Republican efforts to fix\n",
            "the problem. Were having no help, its only obstruction from the Democrats, Trump said. The Democrats are destroying health care in this country.  He added\n",
            "that the eventual bill would be a strictly Republican effort. Republicans or bust, he said.\n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "president            1        0.12%    0.10%    0.02%\n",
            "reporters            1        0.03%    0.01%    0.02%\n",
            "added                1        0.03%    0.02%    0.02%\n",
            "parties              1        0.02%    0.01%    0.01%\n",
            "efforts              1        0.02%    0.01%    0.01%\n",
            "proposal             1        0.02%    0.00%    0.01%\n",
            "saying               1        0.04%    0.03%    0.01%\n",
            "health               2        0.03%    0.02%    0.01%\n",
            "pass                 1        0.02%    0.01%    0.01%\n",
            "called               1        0.04%    0.03%    0.01%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "lives                1        0.01%    0.03%   -0.01%\n",
            "history              1        0.02%    0.03%   -0.01%\n",
            "knows                1        0.01%    0.02%   -0.01%\n",
            "stop                 1        0.02%    0.03%   -0.01%\n",
            "having               1        0.02%    0.02%   -0.01%\n",
            "trying               1        0.02%    0.02%   -0.01%\n",
            "speech               2        0.03%    0.03%   -0.00%\n",
            "wants                1        0.02%    0.02%   -0.00%\n",
            "calling              1        0.01%    0.02%   -0.00%\n",
            "destroying           1        0.00%    0.01%   -0.00%\n",
            "\n",
            "\n",
            "==============================================  Row 1886  ==============================================\n",
            "Gold label: Fake\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "CASTRO DEMANDS THE RETURN OF ILLEGALLY OCCUPIED GITMO BEFORE RESTORING DIPLOMATIC RELATIONS - Obama must have added this to the deal so he can move to close\n",
            "GITMO. He s just itching to release more terrorists and close GITMO because it s clear his legacy is more important than the safety and security of Americans.\n",
            "As President Obama announced that Cuba was restoring full diplomatic relations, reopening embassies in Washington and Havana after more than five decades,\n",
            "Cuba was once again demanding the U.S. return Guantanamo to the country. To achieve normalization [of diplomatic relations] it will be essential also that\n",
            "the territory illegally occupied by the Guantanamo Naval Base is returned,  read a declaration posted on Granma, the official organ of the Communist Party\n",
            "in the island.The Guantanamo Bay Naval Base, home to Guantanamo Bay detention camp since 2012, is located on 45 square miles of a bay the U.S. leased for\n",
            "use as a coaling and naval station in 1903.In the 700-word declaration, the Cuban government also demands the U.S. end the transmission of anti-Castro radio\n",
            "and television broadcasts on the island.The demands echo, almost word by word, those made by President Raul Castro back in January, during the Community\n",
            "of Latin American and Caribbean States summit on Costa Rica. Radio and television transmissions to Cuba ( ) are in violation of international law and are\n",
            "harmful to our sovereignty policies,  the declaration reads, while demanding also that  programs aimed at promoting internal subversion and destabilization\n",
            "be stopped. Obama announced Wednesday that the U.S. and Cuba will reopen their embassies in Havana and Washington, heralding a  new chapter  in relations\n",
            "after a half-century of hostility.  We don t have to be imprisoned by the past,  Obama said from White House Rose Garden.  Americans and Cubans alike are\n",
            "ready to move forward. Read more: FOX NEWS LATINO\n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 1        0.30%    0.09%    0.21%\n",
            "government           1        0.08%    0.05%    0.04%\n",
            "new                  1        0.10%    0.07%    0.03%\n",
            "security             1        0.06%    0.03%    0.03%\n",
            "deal                 1        0.05%    0.02%    0.03%\n",
            "official             1        0.04%    0.02%    0.02%\n",
            "added                1        0.03%    0.02%    0.02%\n",
            "international        1        0.03%    0.02%    0.01%\n",
            "law                  1        0.05%    0.04%    0.01%\n",
            "summit               1        0.01%    0.00%    0.01%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "just                 1        0.04%    0.09%   -0.05%\n",
            "don                  1        0.01%    0.05%   -0.04%\n",
            "read                 1        0.01%    0.03%   -0.02%\n",
            "terrorists           1        0.01%    0.02%   -0.01%\n",
            "posted               1        0.01%    0.02%   -0.01%\n",
            "word                 1        0.01%    0.01%   -0.01%\n",
            "radio                1        0.01%    0.01%   -0.00%\n",
            "use                  1        0.03%    0.03%   -0.00%\n",
            "reopening            1        0.00%    0.00%   -0.00%\n",
            "demanding            2        0.00%    0.01%   -0.00%\n",
            "\n",
            "\n",
            "==============================================  Row 16358  ==============================================\n",
            "Gold label: Fake\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "CASTRO DEMANDS THE RETURN OF ILLEGALLY OCCUPIED GITMO BEFORE RESTORING DIPLOMATIC RELATIONS - Obama must have added this to the deal so he can move to close\n",
            "GITMO. He s just itching to release more terrorists and close GITMO because it s clear his legacy is more important than the safety and security of Americans.\n",
            "As President Obama announced that Cuba was restoring full diplomatic relations, reopening embassies in Washington and Havana after more than five decades,\n",
            "Cuba was once again demanding the U.S. return Guantanamo to the country. To achieve normalization [of diplomatic relations] it will be essential also that\n",
            "the territory illegally occupied by the Guantanamo Naval Base is returned,  read a declaration posted on Granma, the official organ of the Communist Party\n",
            "in the island.The Guantanamo Bay Naval Base, home to Guantanamo Bay detention camp since 2012, is located on 45 square miles of a bay the U.S. leased for\n",
            "use as a coaling and naval station in 1903.In the 700-word declaration, the Cuban government also demands the U.S. end the transmission of anti-Castro radio\n",
            "and television broadcasts on the island.The demands echo, almost word by word, those made by President Raul Castro back in January, during the Community\n",
            "of Latin American and Caribbean States summit on Costa Rica. Radio and television transmissions to Cuba ( ) are in violation of international law and are\n",
            "harmful to our sovereignty policies,  the declaration reads, while demanding also that  programs aimed at promoting internal subversion and destabilization\n",
            "be stopped. Obama announced Wednesday that the U.S. and Cuba will reopen their embassies in Havana and Washington, heralding a  new chapter  in relations\n",
            "after a half-century of hostility.  We don t have to be imprisoned by the past,  Obama said from White House Rose Garden.  Americans and Cubans alike are\n",
            "ready to move forward. Read more: FOX NEWS LATINO\n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 1        0.30%    0.09%    0.21%\n",
            "government           1        0.08%    0.05%    0.04%\n",
            "new                  1        0.10%    0.07%    0.03%\n",
            "security             1        0.06%    0.03%    0.03%\n",
            "deal                 1        0.05%    0.02%    0.03%\n",
            "official             1        0.04%    0.02%    0.02%\n",
            "added                1        0.03%    0.02%    0.02%\n",
            "international        1        0.03%    0.02%    0.01%\n",
            "law                  1        0.05%    0.04%    0.01%\n",
            "summit               1        0.01%    0.00%    0.01%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "just                 1        0.04%    0.09%   -0.05%\n",
            "don                  1        0.01%    0.05%   -0.04%\n",
            "read                 1        0.01%    0.03%   -0.02%\n",
            "terrorists           1        0.01%    0.02%   -0.01%\n",
            "posted               1        0.01%    0.02%   -0.01%\n",
            "word                 1        0.01%    0.01%   -0.01%\n",
            "radio                1        0.01%    0.01%   -0.00%\n",
            "use                  1        0.03%    0.03%   -0.00%\n",
            "reopening            1        0.00%    0.00%   -0.00%\n",
            "demanding            2        0.00%    0.01%   -0.00%\n",
            "\n",
            "\n",
            "==============================================  Row 31076  ==============================================\n",
            "Gold label: Real\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "Trump says he would like Speaker Ryan to chair Republican convention: Fox - WASHINGTON (Reuters) - Donald Trump, the presumptive U.S. Republican presidential\n",
            "nominee, said on Tuesday he would like House of Representatives Speaker Paul Ryan to chair the partys July convention, Fox News Channel reported. He is\n",
            "a very good man. He wants whats good for the party and I think were going to have very positive results. Id love frankly for him to stay and be chairman,\n",
            "Trump said of Ryan in an interview with Fox, according to excerpts released by the television channel. \n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 2        0.30%    0.09%    0.21%\n",
            "party                1        0.07%    0.05%    0.02%\n",
            "presidential         1        0.05%    0.04%    0.01%\n",
            "partys               1        0.01%    0.00%    0.01%\n",
            "television           1        0.01%    0.01%    0.01%\n",
            "says                 1        0.04%    0.04%    0.01%\n",
            "presumptive          1        0.01%    0.00%    0.00%\n",
            "stay                 1        0.01%    0.01%    0.00%\n",
            "positive             1        0.01%    0.01%    0.00%\n",
            "excerpts             1        0.00%    0.00%    0.00%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "like                 2        0.04%    0.08%   -0.03%\n",
            "going                1        0.04%    0.05%   -0.02%\n",
            "love                 1        0.01%    0.02%   -0.01%\n",
            "think                1        0.04%    0.05%   -0.01%\n",
            "good                 2        0.02%    0.04%   -0.01%\n",
            "wants                1        0.02%    0.02%   -0.00%\n",
            "interview            1        0.02%    0.03%   -0.00%\n",
            "released             1        0.02%    0.02%   -0.00%\n",
            "according            1        0.04%    0.04%   -0.00%\n",
            "frankly              1        0.00%    0.00%   -0.00%\n",
            "\n",
            "\n",
            "==============================================  Row 20955  ==============================================\n",
            "Gold label: Real\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "Trump to meet four candidates for FBI director: Spicer - ABOARD AIR FORCE ONE (Reuters) - U.S. President Donald Trump on Wednesday will interview four candidates\n",
            "for the position of director of the Federal Bureau of Investigation, White House spokesman Sean Spicer told reporters. Spicer, who was aboard Air Force\n",
            "One with Trump, said the president would meet with acting FBI Director Andrew McCabe, former Oklahoma Governor Frank Keating, former Connecticut Senator\n",
            "Joe Lieberman and former senior FBI official Richard McFeely. The position was left vacant after Trump fired James Comey. \n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 1        0.30%    0.09%    0.21%\n",
            "spokesman            1        0.04%    0.01%    0.03%\n",
            "told                 1        0.07%    0.04%    0.03%\n",
            "president            1        0.12%    0.10%    0.02%\n",
            "official             1        0.04%    0.02%    0.02%\n",
            "senior               1        0.03%    0.01%    0.02%\n",
            "meet                 2        0.02%    0.01%    0.01%\n",
            "candidates           2        0.02%    0.02%    0.01%\n",
            "director             1        0.03%    0.02%    0.00%\n",
            "position             2        0.02%    0.01%    0.00%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "left                 1        0.02%    0.03%   -0.01%\n",
            "interview            1        0.02%    0.03%   -0.00%\n",
            "fired                1        0.01%    0.01%   -0.00%\n",
            "vacant               1        0.00%    0.00%    0.00%\n",
            "acting               1        0.01%    0.01%    0.00%\n",
            "aboard               1        0.00%    0.00%    0.00%\n",
            "position             2        0.02%    0.01%    0.00%\n",
            "director             1        0.03%    0.02%    0.00%\n",
            "candidates           2        0.02%    0.02%    0.01%\n",
            "meet                 2        0.02%    0.01%    0.01%\n",
            "\n",
            "\n",
            "==============================================  Row 69954  ==============================================\n",
            "Gold label: Real\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "Checking In at Trump Hotels - The New York Times - If you were an alien and had just beamed down to Doral, Fla. the gold letters on a Spanish   building\n",
            "amid 800 acres of golf courses would leave no doubt as to who owns the resort: TRUMP. You might, however, find yourself wondering if this Trump fellow is\n",
            "a reality television star, a titan of industry or a vintner. Trump chardonnay and Trump sauvignon blanc (about $25 each) line the shelves of the Marketplace\n",
            "Cafe at Trump National Doral Miami. Trump nail polish sets ($25) glittery Trump pouches ($30) and Empire by Trump eau de toilette ($62) are sold in a gift\n",
            "shop. A lounge area is replete with framed magazine covers of Donald J. Trump. There he is on Newsweek, mouth agape, pointing at the viewer, beside the\n",
            "words, Youre Fired! There he is in a tuxedo on the cover of Playboy with Brandi Brandt, a former Playmate, who appears to be clad in nothing more than his\n",
            "suit jacket. And there he is gazing at the camera, chin in hand, on the cover of GQ in an issue about men who take risks and make millions.  But it is the\n",
            "2011 cover of a golf magazine, Fairways and Greens, that really grabs the viewer. Mr. Trump, with a golf club at his side, is shown next to the words: President\n",
            "Trump? Turns out, it was a prescient question. Mr. Trumps candidacy has drawn attention not only to his policy positions, but also to everything that bears\n",
            "the Trump name, including about a dozen   hotels and resorts from Waikiki, Honolulu, Hawaii, to Turnberry Ayrshire, Scotland. This year, Trump Hotels  \n",
            "  the hotel company founded in 2007 by Mr. Trump and three of his children (Donald Jr. Ivanka and Eric)     is planning to open Trump International Hotel\n",
            " Tower Vancouver, Trump Hotel Rio de Janeiro and Trump International Hotel, Washington, D. C. In coming years, the company said it will expand to Asia.\n",
            "And next year, it will introduce a lifestyle hotel brand. While the Trump name is ubiquitous, Trump Hotels may not be as familiar to travelers as larger\n",
            "brands with longer histories like Marriott and Hilton. Are Trump Hotels as big, brash and over the top as the man for whom theyre named? In visits to Trump\n",
            "properties in and around New York, Miami and Las Vegas, I set out to see how it feels to be a guest of the man who would be president. As you roll across\n",
            "the paths of Trump National Doral Miami in a golf cart emblazoned with a Trump crest, bird calls alternate with the thwack of golf balls, and every staff\n",
            "members offers a friendly nod or hello. Acre after acre is manicured with   flower beds, orderly rows of palm trees and clipped greens where golf luminaries\n",
            "like Jack Nicklaus and Tiger Woods have trod. A fallen palm frond the size of a surfboard hadnt been on the ground more than a few minutes before an employee\n",
            "pulled up in a golf cart, lifted it onto the roof and drove off, trailing an electric whir, and then, silence. It was a muggy spring afternoon, and the\n",
            "quietude belied the news that had broken days earlier. A staff member said the PGA Tour, which has held an annual tournament at Doral for more than half\n",
            "a century (the Trumps bought the property in 2012) was planning to move it     to Mexico. Mr. Trump, who last year had choice words for Mexican immigrants,\n",
            "broke the news on May 31 that the Tour is moving: Theyre moving it to Mexico City which, by the way, I hope they have kidnapping insurance, he told Fox\n",
            "News. The PGA Tour commissioner said the move was  not a political statement. The next day, however, Ricardo Salinas, the chief executive of the Mexican\n",
            "conglomerate thats moving the tournament, tweeted at Mr. Trump: Youre welcome to join us at the WGC, he wrote, referring to the World Golf Championships.\n",
            "Only good things can turn out, if you know the real Mexico.  Though the PGA Tour may be a thing of the past, Mr. Trump has poured some $250 million into\n",
            "the resorts future. In April, nearly 50 spa suites were opened near the Trump Spa, part of the propertys extensive renovation. The suites, decorated in\n",
            "rich blues with abstract art that evokes the ocean, include soaking tubs use of the Trump Spa lounge areas and, in the evening, Trump wine. Inside the suites\n",
            "(a night in a   in June was $425) one finds Trump Hotel Collection products like bath crystals and a Trump yoga mat bag, one of several amenities to encourage\n",
            "wellness.  Gold is used sparingly: Objets dart and glass boxes for storing jewelry or sunglasses lend an understated glamour. Perhaps its not surprising\n",
            "then that the spa suites were overseen not by Mr. Trump, whose aesthetic is more aligned with opulent hotels in the Middle East and Asia, but by Ivanka\n",
            "Trump, an executive vice president of development and acquisitions for the Trump Organization. Admiring a decorative gold sphere, I turned over a nearby\n",
            "vase to see if it had a mark from a designer like Kelly Wearstler or Jonathan Adler. It had a sticker: West Elm. Maybe someone ought to have peeled it off.\n",
            "But Im glad they didnt: Design trends today mix high and low with abandon one can picture young dcor buffs sharing photos of the suites on Pinterest. The\n",
            "gold sphere was $29 to $39 on WestElm. com cream and gold coasters on the coffee table were $31 for four. A charming bud vase had a CB2 sticker ($10. 95\n",
            "on CB2. com). Generally speaking, the hallways and rooms of Trump Hotels are surprisingly subdued, save for the occasional Trumpian flourish: a gold toilet\n",
            "handle, or chandeliers that appear to have been sized for the Game of Thrones giant Wun Wun. Beyond the gold and glossy exteriors and marble lobbies, the\n",
            "private spaces are   decorated in grays, cream and chocolates. That restraint extends to other aspects of the properties. Music at the pools and restaurants\n",
            "is often, refreshingly, not at full volume. And the hotels are nonsmoking, even in Las Vegas, a city where you can burn your paycheck at any number of hotel\n",
            "casinos. Yet Trump International Hotel Las Vegas doesnt have a casino. Rather, it sells Trump piggy banks for $10. The facade of the Las Vegas property\n",
            "is another matter its the hotel with the unrepentant Midas touch. The building looks like a bar of gold bullion but, after all, its Vegas Mandalay Bay is\n",
            "equally blinding. Make America Great Again baseball caps are proffered in the Trump Store ($30) and adorn the bar at the poolside cafe, H2 Eau, amid bottles\n",
            "of vodka. As long as youre going to be thinking anyway, reads a quote from Mr. Trump splashed across a   mirror at H2 Eau, think big.  The maxim applies\n",
            "to the rooms. My room in Las Vegas, upgraded to a suite at       possibly because the staff is attentive and it was the third time I booked a hotel as a\n",
            "Trump Card member (the brands free loyalty program) or because the hotel figured out where I work (I didnt book using a New York Times email or phone number,\n",
            "though I used my name)     included a   refrigerator, a Wolf stove and a Bosch dishwasher. Even at Trump SoHo New York, the smallest rooms are an impressive\n",
            "420 square feet. The price of admission befits a luxury hotel. The cheapest available room on a July weekend at Trump International Hotel  Tower New York\n",
            "was $563 a night, according to a recent online search. At Trump International Hotel  Tower Chicago, it was $420 a night. That said, rooms are not always\n",
            "out of reach. For instance, one search turned up rooms at Trump International Hotel Las Vegas for $165 a night. Restaurant prices are what one expects at\n",
            "  hotels. At BLT Prime at Trump National Doral Miami, ahi tuna was $37 a   New York strip steak was $51. The hotels also tend to have cafes and shops with\n",
            "affordable sandwiches and snacks. If, however, you get a yen for, say, the bag of gold Trump chocolate bullions in your room at Trump SoHo, it will set\n",
            "you back $35. Sitting at a table in the Trump Bar at Trump Tower New York, the skyscraper used as Wayne Enterprises in the film The Dark Knight Rises, Ivanka\n",
            "Trump said the Trump Hotels customer cant be pigeonholed. We have millennials, she said one spring evening during a reception in conjunction with the N.\n",
            "Y. U. International Hospitality Industry Investment Conference, people from the entertainment business, entrepreneurs, titans of industry and those at the\n",
            "beginning of their careers.  During recent visits to Trump Hotels, I met men and women on vacation or traveling on business, some with families. Guests\n",
            "spoke English, French, Chinese, Hebrew and more. The Trump Hotels website is available in several languages, including Chinese and Arabic, even as Mr. Trump\n",
            "proposes to squeeze China and bar Muslims from entering the United States. Asked to describe their guests, staffers at Trump SoHo said they are chief executives,\n",
            "athletes and actors. The Kardashians have been frequent visitors, though that doesnt necessarily reflect their politics. I know you like the Trump hotel,\n",
            "but honestly how do you feel about Trump running for president? , Khlo Kardashians friend, Malika Haqq, asked her in November on their vlog, Ebony  Ivory.\n",
            "Ms. Kardashian replied: Wha    how does that have to do with the Trump hotel? Well, no, she continued, I dont think he should be president.  Other celebrity\n",
            "guests have chimed in, too. After Mr. Trump said that he would bar Muslims from entering the United States, Lucy Lawless, who played the title role in the\n",
            "television series Xena: Warrior Princess, tweeted: I used to stay in Trump hotels. Can never again, and urged her more than 140, 000 Twitter followers to\n",
            "boycott the brand. Recently, Jimmy Kimmel Live! created a fake commercial for Trump Hotels with the tagline Were not to blame! and apologies to groups Mr.\n",
            "Trump may have offended. Indeed, some companies think Mr. Trumps campaign has hurt his hotel business. Hipmunk, the travel comparison site, said in May\n",
            "that while overall Hipmunk hotel bookings have been on the rise    that has not been the case with bookings of Trump Hotels.  The share of Trump bookings\n",
            "on Hipmunk as a percent of total bookings was down 59 percent year over year. In New York City and Las Vegas, Trump Hotels share as a percentage of each\n",
            "citys total hotel bookings tumbled more than 70 percent. But Trump Hotels executives say the brand has never been stronger. Four of its hotels were among\n",
            "the 0. 4 percent of hotels that made AAAs Five Diamond list this year. Asked at Trump Tower if Mr. Trumps presidential campaign has helped or hurt the hotels,\n",
            "Ivanka Trump said its been an upward trajectory, adding that it may have been the case without the campaign. Behind the    sheets, there has been controversy,\n",
            "from the bankruptcy of Mr. Trumps hotels and casinos in Atlantic City, N. J. to a lawsuit involving Trump SoHo from buyers who alleged they had been defrauded.\n",
            "More recently, Mr. Trump is suing the chefs Jos Andrs and Geoffrey Zakarian, both of whom canceled plans to open restaurants in the Trump International\n",
            "Hotel, Washington, D. C. because of Mr. Trumps comments about Mexican immigrants. Still, the family business rolls on. In the fall the company plans to\n",
            "share details about its new lifestyle hotel brand, as yet unnamed. One name it wont have is Trump. Is that because the name isnt good for business? Ivanka\n",
            "Trump said its because some properties simply arent right for the Trump brand. Its common, in fact, for hotel companies to spawn brands that dont use their\n",
            "name. Marriott, for instance, has nearly 20 brands, including the  . So, are Trump Hotels as splashy as the man for whom theyre named? Only on the surface.\n",
            "Beyond the facades and lobbies theres a feeling of calm and comfort. No smoking. No gaudy colors. Even the service is what one employee called ghost service\n",
            "    omnipresent but silent. Mr. Trump, too, is omnipresent. Just not silent. In guest rooms he appears in videos about Trump Hotels on a television channel\n",
            "in a seemingly   loop. The thing I do best is build, says Mr. Trump during one featuring the forthcoming Trump hotel in the nations capital, set to open\n",
            "weeks before the United States presidential election. Better than The Apprentice,  he says. Better than politics. \n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 10       0.30%    0.09%    0.21%\n",
            "percent              3        0.07%    0.03%    0.04%\n",
            "new                  1        0.10%    0.07%    0.03%\n",
            "told                 1        0.07%    0.04%    0.03%\n",
            "president            1        0.12%    0.10%    0.02%\n",
            "including            3        0.04%    0.02%    0.02%\n",
            "year                 2        0.06%    0.04%    0.02%\n",
            "companies            2        0.03%    0.01%    0.02%\n",
            "executive            2        0.03%    0.01%    0.02%\n",
            "chief                2        0.03%    0.02%    0.02%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "just                 1        0.04%    0.09%   -0.05%\n",
            "com                  1        0.01%    0.06%   -0.05%\n",
            "like                 6        0.04%    0.08%   -0.03%\n",
            "know                 2        0.02%    0.06%   -0.03%\n",
            "right                1        0.03%    0.06%   -0.03%\n",
            "people               1        0.08%    0.10%   -0.03%\n",
            "man                  3        0.02%    0.04%   -0.02%\n",
            "really               1        0.02%    0.04%   -0.02%\n",
            "women                1        0.03%    0.05%   -0.02%\n",
            "thing                2        0.01%    0.03%   -0.02%\n",
            "\n",
            "\n",
            "==============================================  Row 23707  ==============================================\n",
            "Gold label: Real\n",
            "P(Real) = 74%, P(Fake) = 26%\n",
            "===   Text    === \n",
            "Ken Blackwell: Media Borking Sessions Because Hes Southern White, Conservative, Evangelical - Ken Blackwell, former Ohio secretary of state and domestic\n",
            "adviser to Trumps transition team, joined Breitbart News Daily SiriusXM host Raheem Kassam on Thursday to discuss his recent   on the medias attacks on\n",
            "Senator Jeff Sessions after   Donald Trump nominated Sessions for attorney general in his incoming administration. [The Washington Post and L. A. Times\n",
            "spent more than a month working on stories that were exclusively about Senator Sessions and race. Their attempt to define him not by his last twenty years\n",
            "in the Senate, but by the notion that he was white, conservative, evangelical, and from the South meant, in their view, that he could not escape the label\n",
            "of being a racist, said Blackwell.  They dont use facts, he added. They dont use measurable, observable behavior and activity. They, in fact, create this\n",
            "false image of a guy who they would like to bring down because they see him as the tip of the spear of moving us back to a system that respects the rule\n",
            "of law        that respects the Constitution.  Blackwell said, This is an   political attack. This is an effort to misdefine Jeff Sessions in a way that\n",
            "they can destroy him.  This is, he added, pure and simple, the borking of Jeff Sessions. They did it to Judge Bork in the eighties, and theyre trying to\n",
            "do it now to Jeff Sessions.  Blackwell predicted the effort will fail and Sessions will be confirmed. Breitbart News Daily airs on SiriusXM Patriot 125\n",
            "weekdays from 6:00 a. m. to 9:00 a. m. Eastern. \n",
            "\n",
            "=== Top words contributing to P(Real)  ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "said                 1        0.30%    0.09%    0.21%\n",
            "state                1        0.10%    0.06%    0.04%\n",
            "month                1        0.03%    0.01%    0.02%\n",
            "secretary            1        0.04%    0.02%    0.02%\n",
            "general              1        0.04%    0.02%    0.01%\n",
            "adviser              1        0.02%    0.01%    0.01%\n",
            "attacks              1        0.03%    0.02%    0.01%\n",
            "law                  1        0.05%    0.04%    0.01%\n",
            "dont                 2        0.03%    0.02%    0.01%\n",
            "rule                 1        0.02%    0.01%    0.01%\n",
            "\n",
            "=== Top words contributing to P(False) ===\n",
            "Word                 Count    P(Real)  P(Fake)  Delta\n",
            "image                1        0.00%    0.05%   -0.04%\n",
            "like                 1        0.04%    0.08%   -0.03%\n",
            "way                  1        0.03%    0.05%   -0.01%\n",
            "guy                  1        0.01%    0.02%   -0.01%\n",
            "host                 1        0.01%    0.02%   -0.01%\n",
            "destroy              1        0.00%    0.01%   -0.01%\n",
            "false                1        0.01%    0.01%   -0.01%\n",
            "trying               1        0.02%    0.02%   -0.01%\n",
            "behavior             1        0.01%    0.01%   -0.01%\n",
            "stories              1        0.01%    0.01%   -0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating rule clusters"
      ],
      "metadata": {
        "id": "hBh3sijcXIPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Idea: Cluster the embedding of the top & bottom 100 words, interpret categories\n",
        "\n",
        "# Only use words where abs(delta) > x?\n",
        "real_words = nb_lookup.sort_values(by = 'importance_count', ascending = False).iloc[:200].word.values\n",
        "fake_words = nb_lookup.sort_values(by = 'importance_count', ascending = True).iloc[:200].word.values\n",
        "\n",
        "\n",
        "\n",
        "real_vectors = [nlp.vocab.get_vector(word) for word in real_words]\n",
        "fake_vectors = [nlp.vocab.get_vector(word) for word in fake_words]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s-T_nPQVuu80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "sse = {}\n",
        "for n in range(1, 16):\n",
        "  kmeans = KMeans(n_clusters = n, random_state = 0).fit(real_vectors)\n",
        "  sse[n] = kmeans.inertia_\n",
        "\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel('Clusters')\n",
        "plt.ylabel('SSE')\n",
        "plt.title('Real vectors')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "8r0dBEDKcDGn",
        "outputId": "7c1e4673-2aea-4836-98d5-086d1ea93444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Real vectors')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sse = {}\n",
        "for n in range(1, 16):\n",
        "  kmeans = KMeans(n_clusters = n, random_state = 0).fit(fake_vectors)\n",
        "  sse[n] = kmeans.inertia_\n",
        "\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel('Clusters')\n",
        "plt.ylabel('SSE')\n",
        "plt.title('Fake vectors')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "LQRj5jJvhyH7",
        "outputId": "4b9e1b26-1e3d-4dcc-8f8c-eb9f5925478f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Fake vectors')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_real = KMeans(n_clusters = 7, random_state = 0).fit(real_vectors)\n",
        "kmeans_fake = KMeans(n_clusters = 5, random_state = 0).fit(fake_vectors)\n",
        "\n"
      ],
      "metadata": {
        "id": "U4Ahx7PZgkx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_clusters = nb_lookup.sort_values(by = 'importance_count', ascending = False).iloc[:200]\n",
        "real_clusters['cluster'] = kmeans_real.labels_\n",
        "real_clusters['label'] = [1] * len(kmeans_real.labels_)\n",
        "\n",
        "fake_clusters = nb_lookup.sort_values(by = 'importance_count', ascending = True).iloc[:200]\n",
        "fake_clusters['cluster'] = kmeans_fake.labels_\n",
        "fake_clusters['label'] = [0] * len(kmeans_fake.labels_)\n",
        "\n",
        "rule_lookup = pd.concat([real_clusters, fake_clusters])"
      ],
      "metadata": {
        "id": "Pba8j0yhjmvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the dataframe\n",
        "\n",
        "rule_lookup[(rule_lookup.cluster.isin([1,2]) & (rule_lookup.label == 0)) | (rule_lookup.cluster.isin([3]) & (rule_lookup.label == 1))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lVNGciE3mKef",
        "outputId": "9b849246-bd93-4476-ac0c-a8da04821887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-107ac039-905c-4940-b5eb-781d86420fff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>real</th>\n",
              "      <th>fake</th>\n",
              "      <th>delta</th>\n",
              "      <th>importance_tfidf</th>\n",
              "      <th>importance_count</th>\n",
              "      <th>cluster</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>85256</th>\n",
              "      <td>house</td>\n",
              "      <td>-6.901058</td>\n",
              "      <td>-7.590313</td>\n",
              "      <td>0.689255</td>\n",
              "      <td>463.499749</td>\n",
              "      <td>22392.522369</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185899</th>\n",
              "      <td>united</td>\n",
              "      <td>-7.149787</td>\n",
              "      <td>-7.852217</td>\n",
              "      <td>0.702430</td>\n",
              "      <td>366.422655</td>\n",
              "      <td>20828.453177</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126300</th>\n",
              "      <td>north</td>\n",
              "      <td>-7.340915</td>\n",
              "      <td>-8.439673</td>\n",
              "      <td>1.098758</td>\n",
              "      <td>421.751839</td>\n",
              "      <td>15683.666677</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167029</th>\n",
              "      <td>south</td>\n",
              "      <td>-7.885429</td>\n",
              "      <td>-8.917726</td>\n",
              "      <td>1.032296</td>\n",
              "      <td>232.953829</td>\n",
              "      <td>9281.375774</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148955</th>\n",
              "      <td>region</td>\n",
              "      <td>-8.296871</td>\n",
              "      <td>-9.596663</td>\n",
              "      <td>1.299792</td>\n",
              "      <td>181.467077</td>\n",
              "      <td>6262.398183</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178116</th>\n",
              "      <td>thought</td>\n",
              "      <td>-9.033642</td>\n",
              "      <td>-8.560650</td>\n",
              "      <td>-0.472992</td>\n",
              "      <td>-64.524648</td>\n",
              "      <td>-2603.346379</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91402</th>\n",
              "      <td>investigation</td>\n",
              "      <td>-8.187551</td>\n",
              "      <td>-7.936728</td>\n",
              "      <td>-0.250823</td>\n",
              "      <td>-70.501663</td>\n",
              "      <td>-2598.025013</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10177</th>\n",
              "      <td>absolutely</td>\n",
              "      <td>-9.757572</td>\n",
              "      <td>-8.788172</td>\n",
              "      <td>-0.969400</td>\n",
              "      <td>-88.825690</td>\n",
              "      <td>-2586.359912</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95799</th>\n",
              "      <td>js</td>\n",
              "      <td>-12.944291</td>\n",
              "      <td>-9.339105</td>\n",
              "      <td>-3.605186</td>\n",
              "      <td>-137.476189</td>\n",
              "      <td>-2541.655872</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167001</th>\n",
              "      <td>source</td>\n",
              "      <td>-8.741696</td>\n",
              "      <td>-8.344960</td>\n",
              "      <td>-0.396736</td>\n",
              "      <td>-69.421250</td>\n",
              "      <td>-2526.414522</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>161 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-107ac039-905c-4940-b5eb-781d86420fff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-107ac039-905c-4940-b5eb-781d86420fff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-107ac039-905c-4940-b5eb-781d86420fff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 word       real      fake  ...  importance_count  cluster  label\n",
              "85256           house  -6.901058 -7.590313  ...      22392.522369        3      1\n",
              "185899         united  -7.149787 -7.852217  ...      20828.453177        3      1\n",
              "126300          north  -7.340915 -8.439673  ...      15683.666677        3      1\n",
              "167029          south  -7.885429 -8.917726  ...       9281.375774        3      1\n",
              "148955         region  -8.296871 -9.596663  ...       6262.398183        3      1\n",
              "...               ...        ...       ...  ...               ...      ...    ...\n",
              "178116        thought  -9.033642 -8.560650  ...      -2603.346379        1      0\n",
              "91402   investigation  -8.187551 -7.936728  ...      -2598.025013        1      0\n",
              "10177      absolutely  -9.757572 -8.788172  ...      -2586.359912        1      0\n",
              "95799              js -12.944291 -9.339105  ...      -2541.655872        2      0\n",
              "167001         source  -8.741696 -8.344960  ...      -2526.414522        2      0\n",
              "\n",
              "[161 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule testing"
      ],
      "metadata": {
        "id": "xO7fusyOU2aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the predictor"
      ],
      "metadata": {
        "id": "0WkWPllprT4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictor"
      ],
      "metadata": {
        "id": "7dFxywlVsVHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class RulePredictor:\n",
        "\n",
        "  def __init__(self, lookup_df):\n",
        "    self.lookup_df = lookup_df\n",
        "\n",
        "  # Real word --> Vote + 1\n",
        "  # Fake word --> Vote - 1\n",
        "  # If vote = 0 in the end, predict by chance\n",
        "  def predict(self, text):\n",
        "    text = text.lower()\n",
        "    vote = 0\n",
        "    for i, row in self.lookup_df.iterrows():\n",
        "      if row['word'] in text:\n",
        "        vote = vote + 1 if row['label'] == 1 else vote - 1\n",
        "    \n",
        "    if vote == 0:\n",
        "      return int(random.random() > 0.5)\n",
        "    return int(vote > 0)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "Gws5JIuklFdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "CQCsbMAEsMfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def format_time(seconds):\n",
        "  seconds = int(seconds)\n",
        "  m, s = divmod(seconds, 60)\n",
        "  h, m = divmod(m, 60)\n",
        "  return('{:d}:{:02d}:{:02d}'.format(h, m, s))\n",
        "\n",
        "def print_progress(start, it, total_its):\n",
        "  elapsed_time = time.time() - start\n",
        "  done = it / total_its\n",
        "  remaining = format_time(elapsed_time / done - elapsed_time)\n",
        "  \n",
        "  output = '\\n{} iterations of {} completed ({:.0%})'.format(\n",
        "      it, total_its, done\n",
        "      )\n",
        "  output = output + ' in ' + format_time(elapsed_time)\n",
        "  output = output + '\\nEstimated time left: ' + remaining\n",
        "  print(output)"
      ],
      "metadata": {
        "id": "_Hpx1B4F2Yt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-combinations\n",
        "One real and one fake, all combinations\n",
        "Run four times over different subsamples"
      ],
      "metadata": {
        "id": "SHwamLAFW9s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from itertools import product\n",
        "import random\n",
        "\n",
        "real = [0,1,2,3,4,5,6]\n",
        "fake = [0,1,2,3,4]\n",
        "\n",
        "one_combs = list(product(real, fake))\n",
        "\n",
        "f1s = []\n",
        "\n",
        "total_iterations = len(one_combs) * 4\n",
        "current_iteration = 0\n",
        "start = time.time()\n",
        "for i in range(4):\n",
        "  mini = list(zip(x_test, y_test))\n",
        "  random.seed(i)\n",
        "  mini = random.sample(mini, 1000)\n",
        "  x_mini, y_mini = zip(*mini)\n",
        "\n",
        "  f1 = []\n",
        "  for r, f in one_combs:\n",
        "\n",
        "    tmp = rule_lookup[\n",
        "                    (rule_lookup.cluster.isin([f]) & (rule_lookup.label == 0)) | \n",
        "                    (rule_lookup.cluster.isin([r]) & (rule_lookup.label == 1))\n",
        "                    ]\n",
        "\n",
        "    rule_predictor = RulePredictor(tmp)\n",
        "\n",
        "    rule_preds = []\n",
        "    for text in x_mini:\n",
        "      rule_preds.append(rule_predictor.predict(text))\n",
        "\n",
        "    f1.append(f1_score(y_mini, rule_preds, average = 'weighted'))\n",
        "    current_iteration = current_iteration + 1\n",
        "    if current_iteration % 10 == 0:\n",
        "      print_progress(start, current_iteration, total_iterations)\n",
        "  f1s.append(f1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kAsuCBfEn-jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027b078e-aa97-4b57-c379-81e42e823b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 iterations of 140 completed (7%) in 0:00:44\n",
            "Estimated time left: 0:09:38\n",
            "\n",
            "20 iterations of 140 completed (14%) in 0:01:17\n",
            "Estimated time left: 0:07:45\n",
            "\n",
            "30 iterations of 140 completed (21%) in 0:01:54\n",
            "Estimated time left: 0:06:59\n",
            "\n",
            "40 iterations of 140 completed (29%) in 0:02:33\n",
            "Estimated time left: 0:06:24\n",
            "\n",
            "50 iterations of 140 completed (36%) in 0:03:18\n",
            "Estimated time left: 0:05:58\n",
            "\n",
            "60 iterations of 140 completed (43%) in 0:03:51\n",
            "Estimated time left: 0:05:09\n",
            "\n",
            "70 iterations of 140 completed (50%) in 0:04:33\n",
            "Estimated time left: 0:04:33\n",
            "\n",
            "80 iterations of 140 completed (57%) in 0:05:16\n",
            "Estimated time left: 0:03:57\n",
            "\n",
            "90 iterations of 140 completed (64%) in 0:05:49\n",
            "Estimated time left: 0:03:13\n",
            "\n",
            "100 iterations of 140 completed (71%) in 0:06:26\n",
            "Estimated time left: 0:02:34\n",
            "\n",
            "110 iterations of 140 completed (79%) in 0:07:06\n",
            "Estimated time left: 0:01:56\n",
            "\n",
            "120 iterations of 140 completed (86%) in 0:07:52\n",
            "Estimated time left: 0:01:18\n",
            "\n",
            "130 iterations of 140 completed (93%) in 0:08:25\n",
            "Estimated time left: 0:00:38\n",
            "\n",
            "140 iterations of 140 completed (100%) in 0:09:09\n",
            "Estimated time left: 0:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_avg = [(e1 + e2 + e3 + e4) / 4 for (e1, e2, e3, e4) in zip(f1s[0], f1s[1], f1s[2], f1s[3])]\n",
        "\n",
        "res = sorted(zip(one_combs, f1_avg), key = lambda x : x[1], reverse = True)\n",
        "for r in res:\n",
        "  print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ7aKUxtqHRf",
        "outputId": "72a2f991-5d88-4048-f9aa-e49b49606318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((6, 2), 0.7638558942982969)\n",
            "((4, 2), 0.7407355946756246)\n",
            "((2, 2), 0.7245491808041193)\n",
            "((1, 2), 0.7215081117121084)\n",
            "((4, 3), 0.6853140880978399)\n",
            "((4, 0), 0.6763912255909366)\n",
            "((5, 0), 0.673651620294994)\n",
            "((5, 4), 0.644554742191797)\n",
            "((2, 3), 0.6442302710501877)\n",
            "((6, 3), 0.6424665204267201)\n",
            "((5, 3), 0.6345400465549502)\n",
            "((2, 0), 0.632489047573812)\n",
            "((6, 0), 0.6288345039784855)\n",
            "((4, 4), 0.6229387582570257)\n",
            "((5, 2), 0.6141349771290417)\n",
            "((0, 0), 0.6071541420465895)\n",
            "((1, 3), 0.6011176352667829)\n",
            "((3, 0), 0.5962459230747025)\n",
            "((1, 0), 0.5897882791491394)\n",
            "((6, 4), 0.5737150472808232)\n",
            "((0, 3), 0.5721207066630454)\n",
            "((0, 2), 0.569906997916853)\n",
            "((3, 3), 0.5683637187362874)\n",
            "((0, 4), 0.5644063719527765)\n",
            "((3, 4), 0.5599337986233169)\n",
            "((2, 4), 0.5573324647436537)\n",
            "((1, 4), 0.5461355699084629)\n",
            "((3, 2), 0.5290796033324345)\n",
            "((1, 1), 0.4711312362065188)\n",
            "((6, 1), 0.40733581712084865)\n",
            "((4, 1), 0.3453320434733955)\n",
            "((5, 1), 0.3442539379972307)\n",
            "((2, 1), 0.34036859700734046)\n",
            "((0, 1), 0.3395048995422334)\n",
            "((3, 1), 0.3373271877440196)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-combinations\n",
        "Two real, two fake - all combinations\n",
        "Four subsets"
      ],
      "metadata": {
        "id": "CXNi7S8jXIQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "real = [0,1,2,3,4,5,6]\n",
        "fake = [0,1,2,3,4]\n",
        "\n",
        "real_combs = [list(comb) for comb in combinations(real, 2)]\n",
        "fake_combs = [list(comb) for comb in combinations(fake, 2)]\n",
        "\n",
        "two_combs = list(product(real_combs, fake_combs))\n",
        "\n",
        "f1s_2comb = []\n",
        "total_iterations = len(two_combs) * 4\n",
        "current_iteration = 0\n",
        "start = time.time()\n",
        "for i in range(4):\n",
        "  mini = list(zip(x_test, y_test))\n",
        "  random.seed(i)\n",
        "  mini = random.sample(mini, 1000)\n",
        "  x_mini, y_mini = zip(*mini)\n",
        "\n",
        "  f1 = []\n",
        "  for r, f in two_combs:\n",
        "    tmp = rule_lookup[\n",
        "                    (rule_lookup.cluster.isin(f) & (rule_lookup.label == 0)) | \n",
        "                    (rule_lookup.cluster.isin(r) & (rule_lookup.label == 1))\n",
        "                    ]\n",
        "\n",
        "    rule_predictor = RulePredictor(tmp)\n",
        "\n",
        "    rule_preds = []\n",
        "    for text in x_mini:\n",
        "      rule_preds.append(rule_predictor.predict(text))\n",
        "\n",
        "    f1.append(f1_score(y_mini, rule_preds, average = 'weighted'))\n",
        "    current_iteration = current_iteration + 1\n",
        "    if current_iteration % 10 == 0:\n",
        "      print_progress(start, current_iteration, total_iterations)\n",
        "  f1s_2comb.append(f1)\n"
      ],
      "metadata": {
        "id": "5OxTI11_WaJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e7d0d2-37ee-4330-fc0a-526cdd71f8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 iterations of 840 completed (1%) in 0:01:25\n",
            "Estimated time left: 1:58:05\n",
            "\n",
            "20 iterations of 840 completed (2%) in 0:02:32\n",
            "Estimated time left: 1:43:52\n",
            "\n",
            "30 iterations of 840 completed (4%) in 0:03:32\n",
            "Estimated time left: 1:35:41\n",
            "\n",
            "40 iterations of 840 completed (5%) in 0:04:39\n",
            "Estimated time left: 1:33:07\n",
            "\n",
            "50 iterations of 840 completed (6%) in 0:05:48\n",
            "Estimated time left: 1:31:42\n",
            "\n",
            "60 iterations of 840 completed (7%) in 0:07:05\n",
            "Estimated time left: 1:32:05\n",
            "\n",
            "70 iterations of 840 completed (8%) in 0:08:32\n",
            "Estimated time left: 1:33:54\n",
            "\n",
            "80 iterations of 840 completed (10%) in 0:09:53\n",
            "Estimated time left: 1:34:02\n",
            "\n",
            "90 iterations of 840 completed (11%) in 0:11:22\n",
            "Estimated time left: 1:34:46\n",
            "\n",
            "100 iterations of 840 completed (12%) in 0:12:52\n",
            "Estimated time left: 1:35:15\n",
            "\n",
            "110 iterations of 840 completed (13%) in 0:14:30\n",
            "Estimated time left: 1:36:18\n",
            "\n",
            "120 iterations of 840 completed (14%) in 0:15:32\n",
            "Estimated time left: 1:33:17\n",
            "\n",
            "130 iterations of 840 completed (15%) in 0:16:42\n",
            "Estimated time left: 1:31:14\n",
            "\n",
            "140 iterations of 840 completed (17%) in 0:17:53\n",
            "Estimated time left: 1:29:28\n",
            "\n",
            "150 iterations of 840 completed (18%) in 0:19:12\n",
            "Estimated time left: 1:28:21\n",
            "\n",
            "160 iterations of 840 completed (19%) in 0:20:16\n",
            "Estimated time left: 1:26:09\n",
            "\n",
            "170 iterations of 840 completed (20%) in 0:21:21\n",
            "Estimated time left: 1:24:12\n",
            "\n",
            "180 iterations of 840 completed (21%) in 0:22:34\n",
            "Estimated time left: 1:22:47\n",
            "\n",
            "190 iterations of 840 completed (23%) in 0:23:48\n",
            "Estimated time left: 1:21:26\n",
            "\n",
            "200 iterations of 840 completed (24%) in 0:25:09\n",
            "Estimated time left: 1:20:28\n",
            "\n",
            "210 iterations of 840 completed (25%) in 0:26:31\n",
            "Estimated time left: 1:19:35\n",
            "\n",
            "220 iterations of 840 completed (26%) in 0:27:58\n",
            "Estimated time left: 1:18:50\n",
            "\n",
            "230 iterations of 840 completed (27%) in 0:29:05\n",
            "Estimated time left: 1:17:08\n",
            "\n",
            "240 iterations of 840 completed (29%) in 0:30:08\n",
            "Estimated time left: 1:15:21\n",
            "\n",
            "250 iterations of 840 completed (30%) in 0:31:16\n",
            "Estimated time left: 1:13:48\n",
            "\n",
            "260 iterations of 840 completed (31%) in 0:32:25\n",
            "Estimated time left: 1:12:19\n",
            "\n",
            "270 iterations of 840 completed (32%) in 0:33:43\n",
            "Estimated time left: 1:11:12\n",
            "\n",
            "280 iterations of 840 completed (33%) in 0:35:12\n",
            "Estimated time left: 1:10:25\n",
            "\n",
            "290 iterations of 840 completed (35%) in 0:36:33\n",
            "Estimated time left: 1:09:20\n",
            "\n",
            "300 iterations of 840 completed (36%) in 0:38:01\n",
            "Estimated time left: 1:08:26\n",
            "\n",
            "310 iterations of 840 completed (37%) in 0:39:30\n",
            "Estimated time left: 1:07:33\n",
            "\n",
            "320 iterations of 840 completed (38%) in 0:41:10\n",
            "Estimated time left: 1:06:53\n",
            "\n",
            "330 iterations of 840 completed (39%) in 0:42:11\n",
            "Estimated time left: 1:05:12\n",
            "\n",
            "340 iterations of 840 completed (40%) in 0:43:22\n",
            "Estimated time left: 1:03:47\n",
            "\n",
            "350 iterations of 840 completed (42%) in 0:44:33\n",
            "Estimated time left: 1:02:23\n",
            "\n",
            "360 iterations of 840 completed (43%) in 0:45:53\n",
            "Estimated time left: 1:01:11\n",
            "\n",
            "370 iterations of 840 completed (44%) in 0:46:59\n",
            "Estimated time left: 0:59:41\n",
            "\n",
            "380 iterations of 840 completed (45%) in 0:48:06\n",
            "Estimated time left: 0:58:13\n",
            "\n",
            "390 iterations of 840 completed (46%) in 0:49:17\n",
            "Estimated time left: 0:56:52\n",
            "\n",
            "400 iterations of 840 completed (48%) in 0:50:29\n",
            "Estimated time left: 0:55:32\n",
            "\n",
            "410 iterations of 840 completed (49%) in 0:51:47\n",
            "Estimated time left: 0:54:18\n",
            "\n",
            "420 iterations of 840 completed (50%) in 0:53:06\n",
            "Estimated time left: 0:53:06\n",
            "\n",
            "430 iterations of 840 completed (51%) in 0:54:34\n",
            "Estimated time left: 0:52:02\n",
            "\n",
            "440 iterations of 840 completed (52%) in 0:55:43\n",
            "Estimated time left: 0:50:39\n",
            "\n",
            "450 iterations of 840 completed (54%) in 0:56:47\n",
            "Estimated time left: 0:49:13\n",
            "\n",
            "460 iterations of 840 completed (55%) in 0:57:59\n",
            "Estimated time left: 0:47:54\n",
            "\n",
            "470 iterations of 840 completed (56%) in 0:59:09\n",
            "Estimated time left: 0:46:34\n",
            "\n",
            "480 iterations of 840 completed (57%) in 1:00:24\n",
            "Estimated time left: 0:45:18\n",
            "\n",
            "490 iterations of 840 completed (58%) in 1:01:51\n",
            "Estimated time left: 0:44:11\n",
            "\n",
            "500 iterations of 840 completed (60%) in 1:03:12\n",
            "Estimated time left: 0:42:59\n",
            "\n",
            "510 iterations of 840 completed (61%) in 1:04:40\n",
            "Estimated time left: 0:41:51\n",
            "\n",
            "520 iterations of 840 completed (62%) in 1:06:10\n",
            "Estimated time left: 0:40:43\n",
            "\n",
            "530 iterations of 840 completed (63%) in 1:07:46\n",
            "Estimated time left: 0:39:38\n",
            "\n",
            "540 iterations of 840 completed (64%) in 1:08:47\n",
            "Estimated time left: 0:38:13\n",
            "\n",
            "550 iterations of 840 completed (65%) in 1:09:57\n",
            "Estimated time left: 0:36:53\n",
            "\n",
            "560 iterations of 840 completed (67%) in 1:11:09\n",
            "Estimated time left: 0:35:34\n",
            "\n",
            "570 iterations of 840 completed (68%) in 1:12:26\n",
            "Estimated time left: 0:34:18\n",
            "\n",
            "580 iterations of 840 completed (69%) in 1:13:30\n",
            "Estimated time left: 0:32:56\n",
            "\n",
            "590 iterations of 840 completed (70%) in 1:14:34\n",
            "Estimated time left: 0:31:36\n",
            "\n",
            "600 iterations of 840 completed (71%) in 1:15:46\n",
            "Estimated time left: 0:30:18\n",
            "\n",
            "610 iterations of 840 completed (73%) in 1:16:59\n",
            "Estimated time left: 0:29:01\n",
            "\n",
            "620 iterations of 840 completed (74%) in 1:18:18\n",
            "Estimated time left: 0:27:47\n",
            "\n",
            "630 iterations of 840 completed (75%) in 1:19:39\n",
            "Estimated time left: 0:26:33\n",
            "\n",
            "640 iterations of 840 completed (76%) in 1:21:06\n",
            "Estimated time left: 0:25:20\n",
            "\n",
            "650 iterations of 840 completed (77%) in 1:22:13\n",
            "Estimated time left: 0:24:02\n",
            "\n",
            "660 iterations of 840 completed (79%) in 1:23:13\n",
            "Estimated time left: 0:22:41\n",
            "\n",
            "670 iterations of 840 completed (80%) in 1:24:21\n",
            "Estimated time left: 0:21:24\n",
            "\n",
            "680 iterations of 840 completed (81%) in 1:25:31\n",
            "Estimated time left: 0:20:07\n",
            "\n",
            "690 iterations of 840 completed (82%) in 1:26:48\n",
            "Estimated time left: 0:18:52\n",
            "\n",
            "700 iterations of 840 completed (83%) in 1:28:16\n",
            "Estimated time left: 0:17:39\n",
            "\n",
            "710 iterations of 840 completed (85%) in 1:29:40\n",
            "Estimated time left: 0:16:25\n",
            "\n",
            "720 iterations of 840 completed (86%) in 1:31:11\n",
            "Estimated time left: 0:15:11\n",
            "\n",
            "730 iterations of 840 completed (87%) in 1:32:43\n",
            "Estimated time left: 0:13:58\n",
            "\n",
            "740 iterations of 840 completed (88%) in 1:34:22\n",
            "Estimated time left: 0:12:45\n",
            "\n",
            "750 iterations of 840 completed (89%) in 1:35:25\n",
            "Estimated time left: 0:11:27\n",
            "\n",
            "760 iterations of 840 completed (90%) in 1:36:37\n",
            "Estimated time left: 0:10:10\n",
            "\n",
            "770 iterations of 840 completed (92%) in 1:37:49\n",
            "Estimated time left: 0:08:53\n",
            "\n",
            "780 iterations of 840 completed (93%) in 1:39:08\n",
            "Estimated time left: 0:07:37\n",
            "\n",
            "790 iterations of 840 completed (94%) in 1:40:15\n",
            "Estimated time left: 0:06:20\n",
            "\n",
            "800 iterations of 840 completed (95%) in 1:41:22\n",
            "Estimated time left: 0:05:04\n",
            "\n",
            "810 iterations of 840 completed (96%) in 1:42:36\n",
            "Estimated time left: 0:03:48\n",
            "\n",
            "820 iterations of 840 completed (98%) in 1:43:50\n",
            "Estimated time left: 0:02:31\n",
            "\n",
            "830 iterations of 840 completed (99%) in 1:45:11\n",
            "Estimated time left: 0:01:16\n",
            "\n",
            "840 iterations of 840 completed (100%) in 1:46:33\n",
            "Estimated time left: 0:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1s2_avg = [(e1 + e2 + e3 + e4) / 4 for (e1, e2, e3, e4) in zip(f1s_2comb[0], f1s_2comb[1], f1s_2comb[2], f1s_2comb[3])]\n",
        "\n",
        "res = sorted(zip(two_combs, f1s2_avg), key = lambda x : x[1], reverse = True)\n",
        "for r in res:\n",
        "  print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ymD-zX3cAbt",
        "outputId": "41987478-bbb6-4c9c-f099-7ff673841324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(([5, 6], [0, 2]), 0.8046060783854242)\n",
            "(([5, 6], [2, 4]), 0.7994727761697543)\n",
            "(([5, 6], [2, 3]), 0.7986885853130394)\n",
            "(([4, 6], [0, 2]), 0.787823507122177)\n",
            "(([3, 6], [0, 2]), 0.7868604341599276)\n",
            "(([4, 6], [2, 3]), 0.7866098008106941)\n",
            "(([2, 4], [2, 3]), 0.7851811091829518)\n",
            "(([0, 6], [0, 2]), 0.7851109501812259)\n",
            "(([2, 4], [2, 4]), 0.7788873177132518)\n",
            "(([3, 6], [2, 4]), 0.7779641914415792)\n",
            "(([3, 6], [2, 3]), 0.7774284528250021)\n",
            "(([1, 5], [0, 2]), 0.7760931320398206)\n",
            "(([1, 5], [2, 3]), 0.7751281134886947)\n",
            "(([0, 6], [2, 3]), 0.7736303694965752)\n",
            "(([2, 6], [2, 3]), 0.7731878733442064)\n",
            "(([2, 4], [0, 2]), 0.7715386985595127)\n",
            "(([4, 5], [0, 4]), 0.7679357722779374)\n",
            "(([4, 6], [2, 4]), 0.767617503193998)\n",
            "(([0, 6], [2, 4]), 0.7667213786763936)\n",
            "(([2, 6], [0, 2]), 0.7649524427931221)\n",
            "(([1, 3], [2, 3]), 0.7643443077713558)\n",
            "(([4, 5], [2, 4]), 0.7613637783494422)\n",
            "(([2, 5], [2, 4]), 0.7610047606408675)\n",
            "(([1, 5], [2, 4]), 0.7600798378200465)\n",
            "(([5, 6], [0, 3]), 0.7580917893298098)\n",
            "(([4, 5], [3, 4]), 0.7547608476850057)\n",
            "(([0, 1], [2, 3]), 0.7543990896340027)\n",
            "(([1, 3], [0, 2]), 0.7535949717204182)\n",
            "(([0, 1], [0, 2]), 0.7521577466193269)\n",
            "(([1, 4], [2, 3]), 0.7516599855529863)\n",
            "(([2, 5], [0, 2]), 0.7475195815930533)\n",
            "(([4, 5], [0, 2]), 0.7431174053319763)\n",
            "(([0, 6], [0, 3]), 0.7423686826981732)\n",
            "(([2, 5], [2, 3]), 0.7421358683396346)\n",
            "(([4, 5], [0, 3]), 0.7398103660705834)\n",
            "(([1, 4], [0, 2]), 0.7387314175271407)\n",
            "(([1, 2], [2, 3]), 0.7380729832908755)\n",
            "(([2, 6], [2, 4]), 0.7375258270202845)\n",
            "(([3, 6], [0, 3]), 0.737295234651032)\n",
            "(([5, 6], [0, 4]), 0.7357112233562715)\n",
            "(([0, 4], [0, 4]), 0.7348106843252653)\n",
            "(([1, 3], [2, 4]), 0.7344629102639063)\n",
            "(([5, 6], [3, 4]), 0.7335620216161997)\n",
            "(([0, 4], [0, 3]), 0.7324050142494495)\n",
            "(([3, 4], [3, 4]), 0.7323483209387147)\n",
            "(([3, 4], [0, 4]), 0.7280022330247331)\n",
            "(([2, 5], [0, 3]), 0.7271672930363856)\n",
            "(([2, 5], [3, 4]), 0.7269829119004458)\n",
            "(([0, 1], [2, 4]), 0.7256912535814013)\n",
            "(([1, 2], [0, 2]), 0.7233052792926604)\n",
            "(([1, 4], [2, 4]), 0.7220921098474629)\n",
            "(([3, 4], [2, 4]), 0.7199389423355588)\n",
            "(([0, 4], [0, 2]), 0.71924027756222)\n",
            "(([1, 5], [0, 3]), 0.718978303711572)\n",
            "(([4, 5], [2, 3]), 0.7181057879866972)\n",
            "(([0, 4], [3, 4]), 0.718105508977409)\n",
            "(([1, 6], [2, 3]), 0.7178027020486778)\n",
            "(([0, 2], [0, 2]), 0.7160566848116374)\n",
            "(([0, 4], [2, 4]), 0.7149596838157944)\n",
            "(([3, 4], [0, 3]), 0.7134234558021058)\n",
            "(([2, 5], [0, 4]), 0.7112930119484963)\n",
            "(([0, 2], [2, 3]), 0.7109210496616009)\n",
            "(([3, 6], [0, 4]), 0.710647673904615)\n",
            "(([0, 6], [0, 4]), 0.7098620156270623)\n",
            "(([3, 6], [3, 4]), 0.7097764283170486)\n",
            "(([3, 4], [0, 2]), 0.7081915529286951)\n",
            "(([2, 3], [2, 4]), 0.7068079141687708)\n",
            "(([0, 6], [3, 4]), 0.7058651137299434)\n",
            "(([1, 6], [0, 2]), 0.705513563808262)\n",
            "(([2, 3], [0, 2]), 0.7032741015144288)\n",
            "(([1, 2], [2, 4]), 0.7021124354772788)\n",
            "(([0, 4], [2, 3]), 0.7017184797392446)\n",
            "(([2, 4], [0, 3]), 0.7011150129870255)\n",
            "(([0, 2], [2, 4]), 0.6997770978513458)\n",
            "(([0, 2], [0, 3]), 0.6983861704977985)\n",
            "(([1, 5], [3, 4]), 0.6947088367725728)\n",
            "(([3, 4], [2, 3]), 0.6946472756367854)\n",
            "(([2, 3], [2, 3]), 0.6942022949351532)\n",
            "(([1, 3], [0, 3]), 0.6932578135655396)\n",
            "(([1, 5], [0, 4]), 0.6923241282462207)\n",
            "(([0, 1], [0, 3]), 0.6915743312649962)\n",
            "(([1, 6], [2, 4]), 0.6906534363137705)\n",
            "(([4, 6], [0, 3]), 0.6839830455233956)\n",
            "(([2, 3], [3, 4]), 0.683370449083738)\n",
            "(([0, 2], [3, 4]), 0.6816850044110783)\n",
            "(([2, 3], [0, 3]), 0.6800779686096369)\n",
            "(([0, 5], [0, 4]), 0.6778573354762774)\n",
            "(([2, 4], [3, 4]), 0.6734477501959677)\n",
            "(([1, 3], [3, 4]), 0.6723353971136301)\n",
            "(([2, 3], [0, 4]), 0.670831994399476)\n",
            "(([0, 2], [0, 4]), 0.6703476391348074)\n",
            "(([1, 6], [0, 1]), 0.6693845582119811)\n",
            "(([2, 6], [0, 3]), 0.6657892434934478)\n",
            "(([0, 1], [3, 4]), 0.6643341934694955)\n",
            "(([0, 5], [0, 3]), 0.6623932191012655)\n",
            "(([1, 3], [0, 4]), 0.6617039414612707)\n",
            "(([1, 6], [1, 4]), 0.6593753154563746)\n",
            "(([1, 6], [1, 3]), 0.6582494778979091)\n",
            "(([0, 1], [0, 4]), 0.6545814710674023)\n",
            "(([2, 4], [0, 4]), 0.6529420349050864)\n",
            "(([0, 5], [3, 4]), 0.6492431876757935)\n",
            "(([3, 5], [0, 4]), 0.6483411605727134)\n",
            "(([4, 6], [3, 4]), 0.6476787384864563)\n",
            "(([1, 6], [1, 2]), 0.6425268624178052)\n",
            "(([0, 5], [0, 2]), 0.636725947383337)\n",
            "(([1, 4], [0, 3]), 0.6361056728055621)\n",
            "(([1, 2], [0, 3]), 0.6325987832744214)\n",
            "(([3, 5], [0, 3]), 0.6304582723966097)\n",
            "(([0, 5], [2, 4]), 0.6279453494660692)\n",
            "(([2, 6], [3, 4]), 0.6275730020308313)\n",
            "(([4, 6], [0, 4]), 0.6211731009533189)\n",
            "(([3, 5], [3, 4]), 0.6193827792743316)\n",
            "(([0, 5], [2, 3]), 0.6162483285189396)\n",
            "(([2, 6], [0, 4]), 0.6138484283120949)\n",
            "(([0, 3], [0, 4]), 0.6127416973232922)\n",
            "(([1, 4], [3, 4]), 0.6070446534170865)\n",
            "(([3, 5], [0, 2]), 0.6057127990751)\n",
            "(([1, 6], [0, 3]), 0.6037941453765048)\n",
            "(([0, 3], [0, 3]), 0.6007958158251717)\n",
            "(([3, 5], [2, 3]), 0.6000572761331567)\n",
            "(([1, 4], [0, 4]), 0.5936453543223725)\n",
            "(([1, 2], [3, 4]), 0.5914110690676642)\n",
            "(([1, 2], [0, 4]), 0.5878972747683389)\n",
            "(([3, 5], [2, 4]), 0.5853719559527286)\n",
            "(([0, 3], [3, 4]), 0.5832871455447858)\n",
            "(([0, 3], [0, 2]), 0.5824583047561102)\n",
            "(([1, 6], [3, 4]), 0.5794466052183177)\n",
            "(([1, 6], [0, 4]), 0.5751795555521056)\n",
            "(([0, 3], [2, 4]), 0.5652306921570028)\n",
            "(([0, 3], [2, 3]), 0.5631703080215171)\n",
            "(([1, 4], [1, 3]), 0.5616945015165649)\n",
            "(([1, 4], [0, 1]), 0.5585026103086541)\n",
            "(([1, 2], [0, 1]), 0.5479779996468384)\n",
            "(([1, 4], [1, 4]), 0.5472168363500275)\n",
            "(([1, 2], [1, 3]), 0.5453768057105196)\n",
            "(([1, 2], [1, 4]), 0.542753784592493)\n",
            "(([1, 4], [1, 2]), 0.5420155112492706)\n",
            "(([1, 2], [1, 2]), 0.5316701012625342)\n",
            "(([1, 5], [0, 1]), 0.5117858988325862)\n",
            "(([1, 5], [1, 3]), 0.5107541650311362)\n",
            "(([1, 5], [1, 4]), 0.5004249526236191)\n",
            "(([4, 6], [1, 3]), 0.49496544554037636)\n",
            "(([1, 5], [1, 2]), 0.4933117469412347)\n",
            "(([4, 6], [0, 1]), 0.4922843300683042)\n",
            "(([0, 1], [1, 3]), 0.4865531293410928)\n",
            "(([0, 1], [0, 1]), 0.48569877513859777)\n",
            "(([0, 1], [1, 2]), 0.48075887403092943)\n",
            "(([0, 1], [1, 4]), 0.4803903120742866)\n",
            "(([2, 6], [0, 1]), 0.4800608916605115)\n",
            "(([4, 6], [1, 2]), 0.4775283527061348)\n",
            "(([4, 6], [1, 4]), 0.47745949070718763)\n",
            "(([1, 3], [0, 1]), 0.4748845229673871)\n",
            "(([2, 6], [1, 3]), 0.4718980339038292)\n",
            "(([1, 3], [1, 3]), 0.4708196097620198)\n",
            "(([1, 3], [1, 4]), 0.46693520477184286)\n",
            "(([1, 3], [1, 2]), 0.46209539903271096)\n",
            "(([2, 6], [1, 4]), 0.4601584026364417)\n",
            "(([5, 6], [1, 3]), 0.45874068828702425)\n",
            "(([2, 6], [1, 2]), 0.45577511495408213)\n",
            "(([5, 6], [0, 1]), 0.454843988801375)\n",
            "(([5, 6], [1, 2]), 0.44785905701294676)\n",
            "(([0, 6], [1, 3]), 0.4357526294600247)\n",
            "(([5, 6], [1, 4]), 0.43433010930670746)\n",
            "(([0, 6], [1, 2]), 0.4278567433198601)\n",
            "(([0, 6], [0, 1]), 0.4262012520690276)\n",
            "(([0, 6], [1, 4]), 0.4195632871397569)\n",
            "(([3, 6], [1, 3]), 0.41937831156547367)\n",
            "(([3, 6], [0, 1]), 0.4187281729733585)\n",
            "(([3, 6], [1, 2]), 0.412099831291443)\n",
            "(([3, 6], [1, 4]), 0.39457935211418127)\n",
            "(([4, 5], [1, 3]), 0.39380563618699416)\n",
            "(([4, 5], [0, 1]), 0.38986644905216467)\n",
            "(([4, 5], [1, 2]), 0.38846478226610853)\n",
            "(([2, 4], [0, 1]), 0.38540007658622816)\n",
            "(([2, 4], [1, 3]), 0.3831393415914254)\n",
            "(([2, 5], [1, 3]), 0.3727831914246379)\n",
            "(([2, 4], [1, 2]), 0.37192782427241394)\n",
            "(([0, 4], [1, 3]), 0.3687030550622654)\n",
            "(([4, 5], [1, 4]), 0.3673671780934874)\n",
            "(([2, 5], [0, 1]), 0.3666564807238029)\n",
            "(([3, 4], [0, 1]), 0.36591992728126643)\n",
            "(([3, 4], [1, 3]), 0.3648841652055689)\n",
            "(([3, 4], [1, 2]), 0.36457440540195224)\n",
            "(([2, 5], [1, 2]), 0.36361063709769775)\n",
            "(([3, 5], [1, 3]), 0.3611214780159438)\n",
            "(([3, 5], [0, 1]), 0.3611104964842574)\n",
            "(([0, 4], [1, 2]), 0.3608606320684117)\n",
            "(([0, 4], [0, 1]), 0.3607998567723437)\n",
            "(([3, 5], [1, 2]), 0.3582639285193292)\n",
            "(([2, 4], [1, 4]), 0.3578570461743524)\n",
            "(([0, 5], [0, 1]), 0.3568098143209484)\n",
            "(([0, 5], [1, 3]), 0.35563001003153244)\n",
            "(([0, 5], [1, 2]), 0.35450021184187386)\n",
            "(([0, 4], [1, 4]), 0.35257869701182926)\n",
            "(([0, 2], [1, 3]), 0.3522740372184655)\n",
            "(([0, 2], [0, 1]), 0.35192543024620904)\n",
            "(([0, 2], [1, 2]), 0.35154516086509346)\n",
            "(([2, 5], [1, 4]), 0.3502982862332751)\n",
            "(([2, 3], [0, 1]), 0.34976611299714516)\n",
            "(([2, 3], [1, 3]), 0.34891626515228397)\n",
            "(([3, 5], [1, 4]), 0.3481112076147529)\n",
            "(([0, 2], [1, 4]), 0.3466290000790654)\n",
            "(([2, 3], [1, 2]), 0.3454681471446648)\n",
            "(([3, 4], [1, 4]), 0.34453305042704346)\n",
            "(([0, 3], [0, 1]), 0.34369020564137504)\n",
            "(([0, 3], [1, 3]), 0.343162776343541)\n",
            "(([0, 5], [1, 4]), 0.3431400459343866)\n",
            "(([0, 3], [1, 2]), 0.34274864426484697)\n",
            "(([2, 3], [1, 4]), 0.34053724427782733)\n",
            "(([0, 3], [1, 4]), 0.3385431541073443)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Individual clusters\n",
        "Real & fake independantly"
      ],
      "metadata": {
        "id": "jUSjroUjXPxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import random\n",
        "\n",
        "real = [0,1,2,3,4,5,6]\n",
        "fake = [0,1,2,3,4]\n",
        "\n",
        "f1s_real = []\n",
        "f1s_fake = []\n",
        "for i in range(4):\n",
        "  mini = list(zip(x_test, y_test))\n",
        "  random.seed(i)\n",
        "  mini = random.sample(mini, 1000)\n",
        "  x_mini, y_mini = zip(*mini)\n",
        "\n",
        "  f1 = []\n",
        "  for r in real:\n",
        "\n",
        "    tmp = rule_lookup[ \n",
        "                    (rule_lookup.cluster.isin([r]) & (rule_lookup.label == 1))\n",
        "                    ]\n",
        "\n",
        "    rule_predictor = RulePredictor(tmp)\n",
        "\n",
        "    rule_preds = []\n",
        "    for text in x_mini:\n",
        "      rule_preds.append(rule_predictor.predict(text))\n",
        "\n",
        "    f1.append(f1_score(y_mini, rule_preds, average = 'weighted'))\n",
        "  f1s_real.append(f1)\n",
        "\n",
        "  f1 = []\n",
        "  for f in fake:\n",
        "\n",
        "    tmp = rule_lookup[ \n",
        "                    (rule_lookup.cluster.isin([f]) & (rule_lookup.label == 0))\n",
        "                    ]\n",
        "\n",
        "    rule_predictor = RulePredictor(tmp)\n",
        "\n",
        "    rule_preds = []\n",
        "    for text in x_mini:\n",
        "      rule_preds.append(rule_predictor.predict(text))\n",
        "\n",
        "    f1.append(f1_score(y_mini, rule_preds, average = 'weighted'))\n",
        "  f1s_fake.append(f1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zz8TbaVTe42-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_r_avg = [(e1 + e2 + e3 + e4) / 4 for (e1, e2, e3, e4) in zip(f1s_real[0], f1s_real[1], f1s_real[2], f1s_real[3])]\n",
        "f1_f_avg = [(e1 + e2 + e3 + e4) / 4 for (e1, e2, e3, e4) in zip(f1s_fake[0], f1s_fake[1], f1s_fake[2], f1s_fake[3])]\n",
        "\n",
        "res_r = sorted(zip(real, f1_r_avg), key = lambda x : x[1], reverse = True)\n",
        "res_f = sorted(zip(fake, f1_f_avg), key = lambda x : x[1], reverse = True)\n",
        "\n",
        "print('=========== Real results:')\n",
        "for r in res_r:\n",
        "  print(r)\n",
        "\n",
        "print('=========== Fake results:')\n",
        "for r in res_f:\n",
        "  print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjYeAuFpfmF7",
        "outputId": "0446842e-1f11-4f7d-de3b-71dcbc9eb84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========== Real results:\n",
            "(5, 0.5761713421596661)\n",
            "(3, 0.5134456931231718)\n",
            "(0, 0.48407543491916283)\n",
            "(2, 0.4015141990653033)\n",
            "(1, 0.3921649277226231)\n",
            "(6, 0.3888663117808071)\n",
            "(4, 0.37452454118047507)\n",
            "=========== Fake results:\n",
            "(0, 0.3928577213699439)\n",
            "(3, 0.3828577797055899)\n",
            "(2, 0.365429135055805)\n",
            "(1, 0.33500568670661457)\n",
            "(4, 0.333534047192369)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- and 2-combs\n"
      ],
      "metadata": {
        "id": "iy_a69qEXUlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from itertools import product\n",
        "\n",
        "real = [0,1,2,3,4,5,6]\n",
        "fake = [0,1,2,3,4]\n",
        "\n",
        "real_2combs = [list(comb) for comb in combinations(real, 2)]\n",
        "fake_2combs = [list(comb) for comb in combinations(fake, 2)]\n",
        "\n",
        "real_list = [[e] for e in real]\n",
        "fake_list = [[e] for e in fake]\n",
        "\n",
        "tmp = list(product(real_list, fake_2combs))\n",
        "one_two_combs = tmp + list(product(real_2combs, fake_list))"
      ],
      "metadata": {
        "id": "86BuDr7fXXKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1s_2_1_comb = []\n",
        "total_iterations = len(one_two_combs) * 4\n",
        "current_iteration = 0\n",
        "start = time.time()\n",
        "\n",
        "for i in range(4):\n",
        "  mini = list(zip(x_test, y_test))\n",
        "  random.seed(i)\n",
        "  mini = random.sample(mini, 1000)\n",
        "  x_mini, y_mini = zip(*mini)\n",
        "\n",
        "  f1 = []\n",
        "  for r, f in one_two_combs:\n",
        "    tmp = rule_lookup[\n",
        "                    (rule_lookup.cluster.isin(f) & (rule_lookup.label == 0)) | \n",
        "                    (rule_lookup.cluster.isin(r) & (rule_lookup.label == 1))\n",
        "                    ]\n",
        "\n",
        "    rule_predictor = RulePredictor(tmp)\n",
        "\n",
        "    rule_preds = []\n",
        "    for text in x_mini:\n",
        "      rule_preds.append(rule_predictor.predict(text))\n",
        "\n",
        "    f1.append(f1_score(y_mini, rule_preds, average = 'weighted'))\n",
        "    current_iteration = current_iteration + 1\n",
        "    if current_iteration % 10 == 0:\n",
        "      print_progress(start, current_iteration, total_iterations)\n",
        "  f1s_2_1_comb.append(f1)\n"
      ],
      "metadata": {
        "id": "MsH7_43MXeXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa02116-8f39-4cb2-eea0-befc2c0b86a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 iterations of 700 completed (1%) in 0:00:54\n",
            "Estimated time left: 1:02:10\n",
            "\n",
            "20 iterations of 700 completed (3%) in 0:02:09\n",
            "Estimated time left: 1:13:23\n",
            "\n",
            "30 iterations of 700 completed (4%) in 0:03:05\n",
            "Estimated time left: 1:09:05\n",
            "\n",
            "40 iterations of 700 completed (6%) in 0:03:55\n",
            "Estimated time left: 1:04:52\n",
            "\n",
            "50 iterations of 700 completed (7%) in 0:04:53\n",
            "Estimated time left: 1:03:29\n",
            "\n",
            "60 iterations of 700 completed (9%) in 0:05:51\n",
            "Estimated time left: 1:02:32\n",
            "\n",
            "70 iterations of 700 completed (10%) in 0:06:56\n",
            "Estimated time left: 1:02:32\n",
            "\n",
            "80 iterations of 700 completed (11%) in 0:07:50\n",
            "Estimated time left: 1:00:46\n",
            "\n",
            "90 iterations of 700 completed (13%) in 0:08:32\n",
            "Estimated time left: 0:57:51\n",
            "\n",
            "100 iterations of 700 completed (14%) in 0:09:23\n",
            "Estimated time left: 0:56:18\n",
            "\n",
            "110 iterations of 700 completed (16%) in 0:10:27\n",
            "Estimated time left: 0:56:03\n",
            "\n",
            "120 iterations of 700 completed (17%) in 0:11:36\n",
            "Estimated time left: 0:56:07\n",
            "\n",
            "130 iterations of 700 completed (19%) in 0:12:35\n",
            "Estimated time left: 0:55:13\n",
            "\n",
            "140 iterations of 700 completed (20%) in 0:13:25\n",
            "Estimated time left: 0:53:41\n",
            "\n",
            "150 iterations of 700 completed (21%) in 0:14:14\n",
            "Estimated time left: 0:52:14\n",
            "\n",
            "160 iterations of 700 completed (23%) in 0:15:01\n",
            "Estimated time left: 0:50:43\n",
            "\n",
            "170 iterations of 700 completed (24%) in 0:15:55\n",
            "Estimated time left: 0:49:39\n",
            "\n",
            "180 iterations of 700 completed (26%) in 0:16:52\n",
            "Estimated time left: 0:48:45\n",
            "\n",
            "190 iterations of 700 completed (27%) in 0:17:57\n",
            "Estimated time left: 0:48:11\n",
            "\n",
            "200 iterations of 700 completed (29%) in 0:19:02\n",
            "Estimated time left: 0:47:36\n",
            "\n",
            "210 iterations of 700 completed (30%) in 0:19:57\n",
            "Estimated time left: 0:46:33\n",
            "\n",
            "220 iterations of 700 completed (31%) in 0:20:52\n",
            "Estimated time left: 0:45:32\n",
            "\n",
            "230 iterations of 700 completed (33%) in 0:21:52\n",
            "Estimated time left: 0:44:41\n",
            "\n",
            "240 iterations of 700 completed (34%) in 0:22:55\n",
            "Estimated time left: 0:43:56\n",
            "\n",
            "250 iterations of 700 completed (36%) in 0:24:00\n",
            "Estimated time left: 0:43:13\n",
            "\n",
            "260 iterations of 700 completed (37%) in 0:24:42\n",
            "Estimated time left: 0:41:49\n",
            "\n",
            "270 iterations of 700 completed (39%) in 0:25:29\n",
            "Estimated time left: 0:40:36\n",
            "\n",
            "280 iterations of 700 completed (40%) in 0:26:31\n",
            "Estimated time left: 0:39:46\n",
            "\n",
            "290 iterations of 700 completed (41%) in 0:27:35\n",
            "Estimated time left: 0:39:00\n",
            "\n",
            "300 iterations of 700 completed (43%) in 0:28:48\n",
            "Estimated time left: 0:38:25\n",
            "\n",
            "310 iterations of 700 completed (44%) in 0:29:35\n",
            "Estimated time left: 0:37:13\n",
            "\n",
            "320 iterations of 700 completed (46%) in 0:30:29\n",
            "Estimated time left: 0:36:12\n",
            "\n",
            "330 iterations of 700 completed (47%) in 0:31:12\n",
            "Estimated time left: 0:34:59\n",
            "\n",
            "340 iterations of 700 completed (49%) in 0:32:04\n",
            "Estimated time left: 0:33:57\n",
            "\n",
            "350 iterations of 700 completed (50%) in 0:33:03\n",
            "Estimated time left: 0:33:03\n",
            "\n",
            "360 iterations of 700 completed (51%) in 0:33:58\n",
            "Estimated time left: 0:32:05\n",
            "\n",
            "370 iterations of 700 completed (53%) in 0:35:14\n",
            "Estimated time left: 0:31:26\n",
            "\n",
            "380 iterations of 700 completed (54%) in 0:36:11\n",
            "Estimated time left: 0:30:29\n",
            "\n",
            "390 iterations of 700 completed (56%) in 0:37:04\n",
            "Estimated time left: 0:29:27\n",
            "\n",
            "400 iterations of 700 completed (57%) in 0:38:03\n",
            "Estimated time left: 0:28:32\n",
            "\n",
            "410 iterations of 700 completed (59%) in 0:39:06\n",
            "Estimated time left: 0:27:39\n",
            "\n",
            "420 iterations of 700 completed (60%) in 0:40:13\n",
            "Estimated time left: 0:26:49\n",
            "\n",
            "430 iterations of 700 completed (61%) in 0:41:08\n",
            "Estimated time left: 0:25:49\n",
            "\n",
            "440 iterations of 700 completed (63%) in 0:41:50\n",
            "Estimated time left: 0:24:43\n",
            "\n",
            "450 iterations of 700 completed (64%) in 0:42:41\n",
            "Estimated time left: 0:23:42\n",
            "\n",
            "460 iterations of 700 completed (66%) in 0:43:44\n",
            "Estimated time left: 0:22:49\n",
            "\n",
            "470 iterations of 700 completed (67%) in 0:44:54\n",
            "Estimated time left: 0:21:58\n",
            "\n",
            "480 iterations of 700 completed (69%) in 0:45:53\n",
            "Estimated time left: 0:21:01\n",
            "\n",
            "490 iterations of 700 completed (70%) in 0:46:42\n",
            "Estimated time left: 0:20:00\n",
            "\n",
            "500 iterations of 700 completed (71%) in 0:47:33\n",
            "Estimated time left: 0:19:01\n",
            "\n",
            "510 iterations of 700 completed (73%) in 0:48:21\n",
            "Estimated time left: 0:18:00\n",
            "\n",
            "520 iterations of 700 completed (74%) in 0:49:16\n",
            "Estimated time left: 0:17:03\n",
            "\n",
            "530 iterations of 700 completed (76%) in 0:50:14\n",
            "Estimated time left: 0:16:07\n",
            "\n",
            "540 iterations of 700 completed (77%) in 0:51:21\n",
            "Estimated time left: 0:15:13\n",
            "\n",
            "550 iterations of 700 completed (79%) in 0:52:29\n",
            "Estimated time left: 0:14:18\n",
            "\n",
            "560 iterations of 700 completed (80%) in 0:53:24\n",
            "Estimated time left: 0:13:21\n",
            "\n",
            "570 iterations of 700 completed (81%) in 0:54:18\n",
            "Estimated time left: 0:12:23\n",
            "\n",
            "580 iterations of 700 completed (83%) in 0:55:18\n",
            "Estimated time left: 0:11:26\n",
            "\n",
            "590 iterations of 700 completed (84%) in 0:56:22\n",
            "Estimated time left: 0:10:30\n",
            "\n",
            "600 iterations of 700 completed (86%) in 0:57:28\n",
            "Estimated time left: 0:09:34\n",
            "\n",
            "610 iterations of 700 completed (87%) in 0:58:11\n",
            "Estimated time left: 0:08:35\n",
            "\n",
            "620 iterations of 700 completed (89%) in 0:58:59\n",
            "Estimated time left: 0:07:36\n",
            "\n",
            "630 iterations of 700 completed (90%) in 1:00:01\n",
            "Estimated time left: 0:06:40\n",
            "\n",
            "640 iterations of 700 completed (91%) in 1:01:05\n",
            "Estimated time left: 0:05:43\n",
            "\n",
            "650 iterations of 700 completed (93%) in 1:02:19\n",
            "Estimated time left: 0:04:47\n",
            "\n",
            "660 iterations of 700 completed (94%) in 1:03:04\n",
            "Estimated time left: 0:03:49\n",
            "\n",
            "670 iterations of 700 completed (96%) in 1:03:58\n",
            "Estimated time left: 0:02:51\n",
            "\n",
            "680 iterations of 700 completed (97%) in 1:04:41\n",
            "Estimated time left: 0:01:54\n",
            "\n",
            "690 iterations of 700 completed (99%) in 1:05:32\n",
            "Estimated time left: 0:00:56\n",
            "\n",
            "700 iterations of 700 completed (100%) in 1:06:31\n",
            "Estimated time left: 0:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1s2_avg = [(e1 + e2 + e3 + e4) / 4 for (e1, e2, e3, e4) in zip(f1s_2_1_comb[0], f1s_2_1_comb[1], f1s_2_1_comb[2], f1s_2_1_comb[3])]\n",
        "\n",
        "res = sorted(zip(one_two_combs, f1s2_avg), key = lambda x : x[1], reverse = True)\n",
        "for r in res:\n",
        "  print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBw8FvmDZfo-",
        "outputId": "3b85d160-f0b4-421a-c093-74429eb150a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(([4, 5], [2]), 0.7886815747331322)\n",
            "(([2, 5], [2]), 0.7774014929034592)\n",
            "(([3, 4], [2]), 0.7754042702297358)\n",
            "(([6], [0, 2]), 0.7657438080462311)\n",
            "(([0, 4], [2]), 0.7640082003438847)\n",
            "(([6], [2, 4]), 0.7614366050637975)\n",
            "(([1], [2, 3]), 0.7560076999833366)\n",
            "(([5, 6], [2]), 0.7534896878738779)\n",
            "(([6], [2, 3]), 0.747540577514473)\n",
            "(([1], [0, 2]), 0.7461514795643013)\n",
            "(([2, 3], [2]), 0.7456423131598586)\n",
            "(([6], [0, 3]), 0.7432267227382369)\n",
            "(([0, 5], [2]), 0.7417268811487217)\n",
            "(([0, 5], [0]), 0.7413011603166618)\n",
            "(([3, 6], [2]), 0.7383662099183572)\n",
            "(([0, 2], [2]), 0.7376076696297491)\n",
            "(([6], [0, 4]), 0.7359850047779095)\n",
            "(([3, 5], [4]), 0.7292613709391842)\n",
            "(([0, 6], [2]), 0.7283205844456688)\n",
            "(([1], [2, 4]), 0.7261998590329922)\n",
            "(([6], [3, 4]), 0.7243620442948977)\n",
            "(([3, 5], [2]), 0.7154592330293915)\n",
            "(([2, 4], [2]), 0.7122050379134572)\n",
            "(([1, 6], [1]), 0.7118033474732595)\n",
            "(([1, 5], [2]), 0.711360265500856)\n",
            "(([3, 5], [0]), 0.7107208330393714)\n",
            "(([1], [0, 3]), 0.7091733087422274)\n",
            "(([3, 5], [3]), 0.7069786081916443)\n",
            "(([0, 5], [3]), 0.706140050658753)\n",
            "(([1, 3], [2]), 0.7012818466113592)\n",
            "(([4, 5], [3]), 0.6876706937611672)\n",
            "(([1], [0, 4]), 0.6869265056415583)\n",
            "(([1], [3, 4]), 0.6838447950587354)\n",
            "(([0, 1], [2]), 0.6831219160363324)\n",
            "(([0, 3], [0]), 0.6825542081963343)\n",
            "(([0, 3], [2]), 0.6819933388280366)\n",
            "(([0, 5], [4]), 0.6804427972183124)\n",
            "(([4, 6], [2]), 0.6718938394378757)\n",
            "(([4, 5], [0]), 0.67057093681625)\n",
            "(([3, 4], [3]), 0.6678102623530797)\n",
            "(([0, 3], [3]), 0.6622086759926268)\n",
            "(([2, 6], [2]), 0.6569809604327929)\n",
            "(([4], [0, 4]), 0.6550675271173348)\n",
            "(([2, 5], [3]), 0.6531667384590416)\n",
            "(([0, 4], [3]), 0.6467991356388404)\n",
            "(([1, 4], [2]), 0.6450851064118224)\n",
            "(([2, 5], [0]), 0.6410921983422171)\n",
            "(([4], [0, 3]), 0.6390592586888735)\n",
            "(([3, 4], [0]), 0.6385440775313019)\n",
            "(([2], [0, 4]), 0.6382899081094798)\n",
            "(([1, 2], [2]), 0.6355208248741075)\n",
            "(([0, 3], [4]), 0.6327365862786427)\n",
            "(([2], [0, 3]), 0.632470250529139)\n",
            "(([4], [3, 4]), 0.6301955728282906)\n",
            "(([0, 4], [0]), 0.6285496051002953)\n",
            "(([2], [3, 4]), 0.6275343571729335)\n",
            "(([2, 3], [3]), 0.6207586836160266)\n",
            "(([1, 4], [1]), 0.6195746362485451)\n",
            "(([2], [0, 2]), 0.6190993981679634)\n",
            "(([1, 2], [1]), 0.6175682753915297)\n",
            "(([1, 6], [2]), 0.6137926802864333)\n",
            "(([2, 3], [0]), 0.612886890417125)\n",
            "(([5, 6], [3]), 0.612112825491911)\n",
            "(([0, 2], [3]), 0.6088309961927691)\n",
            "(([5, 6], [0]), 0.6030540265879788)\n",
            "(([4], [0, 2]), 0.6011918563854799)\n",
            "(([0, 2], [0]), 0.6011334370057795)\n",
            "(([3, 6], [3]), 0.5988454959886964)\n",
            "(([2], [2, 4]), 0.5972080626207565)\n",
            "(([0, 6], [3]), 0.5943246743268578)\n",
            "(([2], [2, 3]), 0.5918911660067092)\n",
            "(([2, 5], [4]), 0.5913543511167628)\n",
            "(([4, 5], [4]), 0.587327497789395)\n",
            "(([3, 6], [0]), 0.5835972730073566)\n",
            "(([4], [2, 3]), 0.5804478540832755)\n",
            "(([1, 5], [3]), 0.5792500477548959)\n",
            "(([0, 6], [0]), 0.5763994814087863)\n",
            "(([1, 5], [0]), 0.5751730363068304)\n",
            "(([4], [2, 4]), 0.5694181462086251)\n",
            "(([1, 5], [1]), 0.569306997113688)\n",
            "(([4, 6], [1]), 0.5641048313648633)\n",
            "(([1, 3], [0]), 0.561938121777385)\n",
            "(([1, 3], [3]), 0.5602334292167066)\n",
            "(([3, 4], [4]), 0.5588105436918837)\n",
            "(([2, 6], [1]), 0.5571838635278549)\n",
            "(([5, 6], [4]), 0.55386095302759)\n",
            "(([0, 1], [1]), 0.5529805934219997)\n",
            "(([0, 4], [4]), 0.5471807038073225)\n",
            "(([0, 6], [4]), 0.5467715556325662)\n",
            "(([0, 1], [3]), 0.5451122599689996)\n",
            "(([0, 1], [0]), 0.5431709151088653)\n",
            "(([2, 3], [4]), 0.5402516593917157)\n",
            "(([3, 6], [4]), 0.5384215698366268)\n",
            "(([1, 5], [4]), 0.5380031710364623)\n",
            "(([2, 4], [3]), 0.5377828410520339)\n",
            "(([0, 2], [4]), 0.537314507776789)\n",
            "(([1, 3], [1]), 0.5328086088945032)\n",
            "(([2, 4], [0]), 0.5277756467456213)\n",
            "(([5], [0, 3]), 0.5270391785504598)\n",
            "(([1, 3], [4]), 0.5197262922087155)\n",
            "(([0, 1], [4]), 0.5192431396189606)\n",
            "(([5, 6], [1]), 0.5072959167485113)\n",
            "(([2, 6], [0]), 0.49942008609569644)\n",
            "(([2, 6], [3]), 0.49907665524691563)\n",
            "(([5], [0, 4]), 0.494982021292901)\n",
            "(([4, 6], [3]), 0.4931144626249879)\n",
            "(([1, 2], [0]), 0.4901905749882984)\n",
            "(([0, 6], [1]), 0.488107552522939)\n",
            "(([5], [2, 3]), 0.4845728671457493)\n",
            "(([5], [0, 2]), 0.4828949619076398)\n",
            "(([5], [3, 4]), 0.48182067631726133)\n",
            "(([1, 4], [0]), 0.4777969253073745)\n",
            "(([0], [0, 3]), 0.4769693379872397)\n",
            "(([1, 6], [3]), 0.47422616429451625)\n",
            "(([1, 2], [3]), 0.4740775742963522)\n",
            "(([4, 6], [0]), 0.4739842220392224)\n",
            "(([0], [0, 4]), 0.4714304050979183)\n",
            "(([1, 6], [0]), 0.4683090238313895)\n",
            "(([2, 6], [4]), 0.4665044658369708)\n",
            "(([1, 4], [3]), 0.4645608875799114)\n",
            "(([3, 6], [1]), 0.4634621192704784)\n",
            "(([1, 6], [4]), 0.46230744833024456)\n",
            "(([0], [2, 4]), 0.46165742643815305)\n",
            "(([0], [0, 2]), 0.4612040836520712)\n",
            "(([0], [3, 4]), 0.4607262865894907)\n",
            "(([0], [2, 3]), 0.4575566222572669)\n",
            "(([1, 2], [4]), 0.455389208852265)\n",
            "(([3], [0, 3]), 0.45479438771630637)\n",
            "(([2, 4], [4]), 0.4516271328410265)\n",
            "(([4, 6], [4]), 0.44996957393920545)\n",
            "(([5], [2, 4]), 0.4411488917320843)\n",
            "(([1, 4], [4]), 0.43902954165582253)\n",
            "(([3], [0, 4]), 0.4385902687579852)\n",
            "(([3], [0, 2]), 0.436230389021391)\n",
            "(([1], [0, 1]), 0.42279418255716433)\n",
            "(([4, 5], [1]), 0.42194069235345694)\n",
            "(([3], [2, 3]), 0.41944293183886866)\n",
            "(([1], [1, 3]), 0.41735127157455987)\n",
            "(([1], [1, 2]), 0.41721637342255014)\n",
            "(([2, 4], [1]), 0.41670501335449334)\n",
            "(([1], [1, 4]), 0.41186457944484955)\n",
            "(([3], [3, 4]), 0.3991641343481683)\n",
            "(([2, 5], [1]), 0.3876414171756283)\n",
            "(([3], [2, 4]), 0.3859651883069437)\n",
            "(([3, 4], [1]), 0.38455370202386435)\n",
            "(([0, 4], [1]), 0.38368416508856484)\n",
            "(([6], [1, 3]), 0.38175298049718653)\n",
            "(([6], [0, 1]), 0.3772643357833394)\n",
            "(([6], [1, 2]), 0.37489650690530546)\n",
            "(([3, 5], [1]), 0.3698500568178394)\n",
            "(([0, 2], [1]), 0.3691093255314327)\n",
            "(([0, 5], [1]), 0.36724264401735085)\n",
            "(([2, 3], [1]), 0.3660766030140622)\n",
            "(([6], [1, 4]), 0.3604327986107984)\n",
            "(([0, 3], [1]), 0.3481421990661616)\n",
            "(([4], [1, 2]), 0.34588306046906736)\n",
            "(([4], [1, 3]), 0.3456964326113637)\n",
            "(([4], [0, 1]), 0.34547283570300313)\n",
            "(([5], [0, 1]), 0.34514219093761594)\n",
            "(([5], [1, 3]), 0.3434706201222406)\n",
            "(([5], [1, 2]), 0.34252529971262546)\n",
            "(([0], [1, 2]), 0.33965976420469035)\n",
            "(([2], [0, 1]), 0.33897023789333547)\n",
            "(([5], [1, 4]), 0.3385429015190059)\n",
            "(([2], [1, 2]), 0.3385421937420604)\n",
            "(([0], [1, 3]), 0.33820266053295717)\n",
            "(([0], [0, 1]), 0.3379916839092965)\n",
            "(([4], [1, 4]), 0.33764416600421254)\n",
            "(([3], [1, 2]), 0.33756104789351266)\n",
            "(([3], [1, 3]), 0.33687488769232277)\n",
            "(([2], [1, 3]), 0.33666412270585894)\n",
            "(([0], [1, 4]), 0.3365542408208109)\n",
            "(([2], [1, 4]), 0.3362229866932511)\n",
            "(([3], [0, 1]), 0.33622230325450303)\n",
            "(([3], [1, 4]), 0.33567595900631847)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpreting clusters"
      ],
      "metadata": {
        "id": "2CFAfiTgK_ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seeing the interia for each cluster"
      ],
      "metadata": {
        "id": "xEbeCmeHM8x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_r = KMeans(n_clusters = 7, random_state = 0)\n",
        "kmeans_f = KMeans(n_clusters = 5, random_state = 0)\n",
        "distances_r = kmeans_r.fit_transform(real_vectors)\n",
        "distances_f = kmeans_f.fit_transform(fake_vectors)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QMD1twVXM0eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_clusters = list(set(kmeans_r.labels_))\n",
        "f_clusters = list(set(kmeans_f.labels_))\n",
        "\n",
        "variance_r = dict.fromkeys(r_clusters, 0)\n",
        "i = 0\n",
        "for label in kmeans_r.labels_:\n",
        "    variance_r[label] = variance_r[label] + distances_r[i][label]\n",
        "    i = i + 1\n",
        "\n",
        "variance_f = dict.fromkeys(f_clusters, 0)\n",
        "i = 0\n",
        "for label in kmeans_f.labels_:\n",
        "    variance_f[label] = variance_f[label] + distances_f[i][label]\n",
        "    i = i + 1"
      ],
      "metadata": {
        "id": "ZhLlV0guNpjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print('Real clusters:')\n",
        "for k, v in variance_r.items():\n",
        "  print('Cluster {}: {:>7}'.format(k, round(v, 2)))\n",
        "\n",
        "print('\\nFake clusters:')\n",
        "for k, v in variance_f.items():\n",
        "  print('Cluster {}: {:>7}'.format(k, round(v, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IP0DdsTNofT",
        "outputId": "266ee8d5-e71a-4604-82de-517919e5fc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real clusters:\n",
            "Cluster 0:   87.23\n",
            "Cluster 1:  301.99\n",
            "Cluster 2:  101.98\n",
            "Cluster 3:   51.29\n",
            "Cluster 4:  122.46\n",
            "Cluster 5:  138.76\n",
            "Cluster 6:  198.73\n",
            "\n",
            "Fake clusters:\n",
            "Cluster 0:   122.3\n",
            "Cluster 1:  460.64\n",
            "Cluster 2:  210.41\n",
            "Cluster 3:  136.69\n",
            "Cluster 4:   10.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Studying clusters"
      ],
      "metadata": {
        "id": "DM71_K1E0sVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Study the clusters!\n",
        "tmp = rule_lookup[((rule_lookup.cluster == 6) & (rule_lookup.label == 1))].sort_values(by = 'importance_count', ascending = False)\n",
        "print(len(tmp))\n",
        "print(tmp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SlBJp7w2biG",
        "outputId": "8f3bb75f-de7c-4362-c9e0-b2d983998ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "                   word      real       fake  ...  importance_count  cluster  label\n",
            "155877             said -5.805900  -7.020278  ...     199488.303753        6      1\n",
            "116800         minister -7.457444  -9.272784  ...      18269.579309        6      1\n",
            "179627             told -7.337762  -7.881266  ...      14260.477397        6      1\n",
            "128650        officials -7.558363  -8.423673  ...      14231.740542        6      1\n",
            "141242        president -6.733552  -6.951393  ...      13667.747668        6      1\n",
            "45170             court -7.352300  -8.137441  ...      13524.827929        6      1\n",
            "169119        statement -7.599916  -8.461944  ...      11039.994586        6      1\n",
            "167930        spokesman -7.948490  -9.750158  ...      10175.818920        6      1\n",
            "133925            party -7.308720  -7.684401  ...       9167.009072        6      1\n",
            "174376            talks -7.981968  -9.844971  ...       8914.469658        6      1\n",
            "104040          leaders -7.899927  -8.889643  ...       8772.836339        6      1\n",
            "41734         committee -7.777874  -8.670569  ...       8674.313641        6      1\n",
            "128641         official -7.876211  -8.682695  ...       8031.778944        6      1\n",
            "104032           leader -7.876826  -8.724240  ...       7930.943545        6      1\n",
            "150291        reporters -8.010430  -9.198132  ...       7078.705201        6      1\n",
            "12334            agency -8.061472  -9.016210  ...       6818.736677        6      1\n",
            "37985             chief -8.041781  -8.768330  ...       6659.548742        6      1\n",
            "160240           senior -8.203608  -9.208421  ...       6625.736805        6      1\n",
            "64041         executive -8.109151  -8.922122  ...       6368.812007        6      1\n",
            "114132          meeting -7.889815  -8.607921  ...       6212.334390        6      1\n",
            "185842            union -8.159275  -9.150849  ...       6185.438609        6      1\n",
            "159461        secretary -7.816861  -8.314357  ...       5723.190802        6      1\n",
            "116821         ministry -8.295599 -10.059361  ...       5638.748273        6      1\n",
            "49605          decision -8.004520  -8.623485  ...       5506.314235        6      1\n",
            "150369  representatives -8.411465  -9.954800  ...       4906.262172        6      1\n",
            "104286              led -8.351044  -8.986869  ...       4551.872891        6      1\n",
            "154785           ruling -8.437693  -9.571529  ...       4489.987171        6      1\n",
            "51148        department -7.759519  -8.061483  ...       4342.541727        6      1\n",
            "128606           office -7.817860  -8.109961  ...       4325.146274        6      1\n",
            "150366   representative -8.566709  -9.818226  ...       4213.855258        6      1\n",
            "114573          members -8.051478  -8.442181  ...       4204.754544        6      1\n",
            "42610        conference -8.273021  -9.041163  ...       4060.397545        6      1\n",
            "36777          chairman -8.434566  -9.222571  ...       3761.935628        6      1\n",
            "30915          briefing -8.550007 -10.140896  ...       3756.090071        6      1\n",
            "12543            agreed -8.578288  -9.625584  ...       3726.277849        6      1\n",
            "41715        commission -8.610697  -9.785598  ...       3710.338206        6      1\n",
            "167942      spokeswoman -8.695751 -10.382043  ...       3682.861949        6      1\n",
            "11805           adviser -8.611598  -9.643423  ...       3383.352458        6      1\n",
            "44910           council -8.426910  -9.100164  ...       3308.372465        6      1\n",
            "82090              held -8.375718  -8.857588  ...       3252.144217        6      1\n",
            "\n",
            "[40 rows x 8 columns]\n"
          ]
        }
      ]
    }
  ]
}